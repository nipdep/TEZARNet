{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random \n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F \n",
    "from torch.optim import Adam\n",
    "\n",
    "from scipy.signal import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build PAMAP2 dataset data reader\n",
    "class PAMAP2Reader(object):\n",
    "    def __init__(self, root_path):\n",
    "        self.root_path = root_path\n",
    "        self.readPamap2()\n",
    "\n",
    "    def readFile(self, file_path, cols):\n",
    "        all_data = {\"data\": {}, \"target\": {}, 'collection': []}\n",
    "        prev_action = -1\n",
    "        starting = True\n",
    "        # action_seq = []\n",
    "        action_ID = 0\n",
    "\n",
    "        for l in open(file_path).readlines():\n",
    "            s = l.strip().split()\n",
    "            if s[1] != \"0\":\n",
    "                if (prev_action != int(s[1])):\n",
    "                    if not(starting):\n",
    "                        df = pd.DataFrame(action_seq)\n",
    "                        intep_df = df.interpolate(method='linear', limit_direction='both', axis=0)\n",
    "                        intep_data = intep_df.values \n",
    "                        all_data['data'][action_ID] = np.array(intep_data)\n",
    "                        all_data['target'][action_ID] = prev_action\n",
    "                        action_ID+=1\n",
    "                    action_seq = []\n",
    "                else:\n",
    "                    starting = False\n",
    "                data_seq = np.array(s[3:])[cols].astype(np.float16)\n",
    "                # data_seq[np.isnan(data_seq)] = 0\n",
    "                action_seq.append(data_seq)\n",
    "                prev_action = int(s[1])\n",
    "                # print(prev_action)\n",
    "                all_data['collection'].append(data_seq)\n",
    "        else: \n",
    "            if len(action_seq) > 1:\n",
    "                df = pd.DataFrame(action_seq)\n",
    "                intep_df = df.interpolate(method='linear', limit_direction='both', axis=0)\n",
    "                intep_data = intep_df.values\n",
    "                all_data['data'][action_ID] = np.array(intep_data)\n",
    "                all_data['target'][action_ID] = prev_action\n",
    "        return all_data\n",
    "\n",
    "    def readPamap2Files(self, filelist, cols, labelToId):\n",
    "        data = []\n",
    "        labels = []\n",
    "        collection = []\n",
    "        for i, filename in enumerate(filelist):\n",
    "            print('Reading file %d of %d' % (i+1, len(filelist)))\n",
    "            fpath = os.path.join(self.root_path, filename)\n",
    "            file_data = self.readFile(fpath, cols)\n",
    "            data.extend(list(file_data['data'].values()))\n",
    "            labels.extend(list(file_data['target'].values()))\n",
    "            collection.extend(file_data['collection'])\n",
    "        return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n",
    "\n",
    "    def readPamap2(self):\n",
    "        files = ['subject101.dat', 'subject102.dat','subject103.dat','subject104.dat', 'subject105.dat', 'subject106.dat', 'subject107.dat', 'subject108.dat', 'subject109.dat', 'subject110.dat', 'subject111.dat', 'subject112.dat', 'subject113.dat', 'subject114.dat']\n",
    "            \n",
    "        label_map = [\n",
    "            (0, 'other'),\n",
    "            (1, 'lying'),\n",
    "            (2, 'sitting'),\n",
    "            (3, 'standing'),\n",
    "            (4, 'walking'),\n",
    "            (5, 'running'),\n",
    "            (6, 'cycling'),\n",
    "            (7, 'Nordic walking'),\n",
    "            (9, 'watching TV'),\n",
    "            (10, 'computer work'),\n",
    "            (11, 'car driving'),\n",
    "            (12, 'ascending stairs'),\n",
    "            (13, 'descending stairs'),\n",
    "            (16, 'vacuum cleaning'),\n",
    "            (17, 'ironing'),\n",
    "            (18, 'folding laundry'),\n",
    "            (19, 'house cleaning'),\n",
    "            (20, 'playing soccer'),\n",
    "            (24, 'rope jumping')\n",
    "        ]\n",
    "        labelToId = {x[0]: i for i, x in enumerate(label_map)}\n",
    "        # print \"label2id=\",labelToId\n",
    "        idToLabel = [x[1] for x in label_map]\n",
    "        # print \"id2label=\",idToLabel\n",
    "        cols = [1,2,3,7,8,9,17,18,19,23,24,25,33,34,35,39,40,41]\n",
    "        # print \"cols\",cols\n",
    "        self.data, self.targets, self.all_data = self.readPamap2Files(files, cols, labelToId)\n",
    "        # print(self.data)\n",
    "        # nan_perc = np.isnan(self.data).astype(int).mean()\n",
    "        # print(\"null value percentage \", nan_perc)\n",
    "        # f = lambda x: labelToId[x]\n",
    "        print(np.unique(self.targets))\n",
    "        self.targets = np.array([labelToId[i] for i in list(self.targets)])\n",
    "        print(np.unique(self.targets))\n",
    "        self.label_map = label_map\n",
    "        self.idToLabel = idToLabel\n",
    "        # return data, idToLabel\n",
    "\n",
    "    def aggregate(self, signal):\n",
    "        # print(signal.min(), signal.max())\n",
    "        means = signal.astype(np.float64).mean(axis=0)\n",
    "        stds = signal.astype(np.float64).std(axis=0)\n",
    "        if np.isinf(stds).sum() > 0:\n",
    "            # print(stds, signal)\n",
    "            pass\n",
    "        mergered = np.vstack((means,stds)).reshape((-1,),order='F')\n",
    "        # print(signal.shape, means.shape, stds.shape, mergered.shape)\n",
    "        return mergered\n",
    "\n",
    "    def windowing(self, signal, window_len, overlap):\n",
    "        seq_len = int(window_len*100) # 100Hz compensation \n",
    "        overlap_len = int(overlap*100) # 100Hz\n",
    "        l, _ = signal.shape\n",
    "        if l > seq_len:\n",
    "            windowing_points = np.arange(start=0, stop=l-seq_len, step=seq_len-overlap_len, dtype=int)[:-1]\n",
    "            # windowing_points = windowing_points-overlap_len\n",
    "            # windowing_points[0] = 0 \n",
    "\n",
    "            windows = [signal[p:p+seq_len, :] for p in windowing_points]\n",
    "        else:\n",
    "            windows = []\n",
    "        return windows\n",
    "\n",
    "    def resampling(self, data, targets, window_size, window_overlap, resample_freq):\n",
    "        assert len(data) == len(targets), \"# action data & # action labels are not matching\"\n",
    "        all_data, all_ids, all_labels = [], [], []\n",
    "        for i, d in enumerate(data):\n",
    "            # print(\">>>>>>>>>>>>>>>  \", np.isnan(d).mean())\n",
    "            label = targets[i]\n",
    "            windows = self.windowing(d, window_size, window_overlap)\n",
    "            for w in windows:\n",
    "                # print(np.isnan(w).mean(), label, i)\n",
    "                resample_sig = self.aggregate(w)\n",
    "                # print(np.isnan(resample_sig).mean(), label, i)\n",
    "                all_data.append(resample_sig)\n",
    "                all_ids.append(i+1)\n",
    "                all_labels.append(label)\n",
    "\n",
    "        return all_data, all_ids, all_labels\n",
    "\n",
    "    def generate(self, unseen_classes, resampling=True, window_size=5.21, window_overlap=1, resample_freq=10, seen_ratio=0.2, unseen_ratio=0.8):\n",
    "        # assert all([i in list(self.label_map.keys()) for i in unseen_classes]), \"Unknown Class label!\"\n",
    "        seen_classes = [i for i in range(len(self.idToLabel)) if i not in unseen_classes]\n",
    "        unseen_mask = np.in1d(self.targets, unseen_classes)\n",
    "\n",
    "        s = np.unique(self.targets, return_counts=True)\n",
    "        print(\"per class count : \", dict(zip([self.idToLabel[i] for i in s[0]], s[1])))\n",
    "\n",
    "        # build seen dataset \n",
    "        seen_data = self.data[np.invert(unseen_mask)]\n",
    "        seen_targets = self.targets[np.invert(unseen_mask)]\n",
    "\n",
    "        # print('>>>>  ', seen_data.min(), seen_data.max())\n",
    "\n",
    "        # build unseen dataset\n",
    "        unseen_data = self.data[unseen_mask]\n",
    "        unseen_targets = self.targets[unseen_mask]\n",
    "\n",
    "        # resampling seen and unseen datasets \n",
    "        seen_data, seen_ids, seen_targets = self.resampling(seen_data, seen_targets, window_size, window_overlap, resample_freq)\n",
    "        unseen_data, unseen_ids, unseen_targets = self.resampling(unseen_data, unseen_targets, window_size, window_overlap, resample_freq)\n",
    "\n",
    "        seen_data, seen_targets = np.array(seen_data), np.array(seen_targets)\n",
    "        unseen_data, unseen_targets = np.array(unseen_data), np.array(unseen_targets)\n",
    "        \n",
    "        # pos_thr = 10e4\n",
    "        # pos_mask = seen_data<pos_thr\n",
    "        # seen_data = seen_data[pos_mask]\n",
    "        # seen_targets = seen_targets[pos_mask]\n",
    "        print('>> ', seen_data.min(), seen_data.max())\n",
    "        # train-val split\n",
    "        seen_index = list(range(len(seen_targets)))\n",
    "        random.shuffle(seen_index)\n",
    "        split_point = int((1-seen_ratio)*len(seen_index))\n",
    "        fst_index, sec_index = seen_index[:split_point], seen_index[split_point:]\n",
    "        print(type(fst_index), type(sec_index), type(seen_data), type(seen_targets))\n",
    "        X_seen_train, X_seen_val, y_seen_train, y_seen_val = seen_data[fst_index,:], seen_data[sec_index,:], seen_targets[fst_index], seen_targets[sec_index]\n",
    "        \n",
    "        # val-test split\n",
    "        unseen_index = list(range(len(unseen_targets)))\n",
    "        random.shuffle(unseen_index)\n",
    "        split_point = int((1-unseen_ratio)*len(unseen_index))\n",
    "        fst_index, sec_index = unseen_index[:split_point], unseen_index[split_point:]\n",
    "\n",
    "        X_unseen_val, X_unseen_test, y_unseen_val, y_unseen_test = unseen_data[fst_index,:], unseen_data[sec_index,:], unseen_targets[fst_index], unseen_targets[sec_index]\n",
    "\n",
    "        data = {'train': {\n",
    "                        'X': X_seen_train,\n",
    "                        'y': y_seen_train\n",
    "                        },\n",
    "                'eval-seen':{\n",
    "                        'X': X_seen_val,\n",
    "                        'y': y_seen_val\n",
    "                        },\n",
    "                'test': {\n",
    "                        'X': unseen_data,\n",
    "                        'y': unseen_targets\n",
    "                        },\n",
    "                'seen_classes': seen_classes,\n",
    "                'unseen_classes': unseen_classes\n",
    "                }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 1 of 14\n",
      "Reading file 2 of 14\n",
      "Reading file 3 of 14\n",
      "Reading file 4 of 14\n",
      "Reading file 5 of 14\n",
      "Reading file 6 of 14\n",
      "Reading file 7 of 14\n",
      "Reading file 8 of 14\n",
      "Reading file 9 of 14\n",
      "Reading file 10 of 14\n",
      "Reading file 11 of 14\n",
      "Reading file 12 of 14\n",
      "Reading file 13 of 14\n",
      "Reading file 14 of 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deela\\AppData\\Local\\Temp\\ipykernel_39460\\1588132582.py:54: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  9 10 11 12 13 16 17 18 19 20 24]\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]\n"
     ]
    }
   ],
   "source": [
    "dataReader = PAMAP2Reader('../data/PAMAP2_Dataset/Protocol/')\n",
    "actionList = dataReader.idToLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per class count :  {'lying': 8, 'sitting': 8, 'standing': 8, 'walking': 8, 'running': 6, 'cycling': 7, 'Nordic walking': 7, 'watching TV': 1, 'computer work': 4, 'car driving': 1, 'ascending stairs': 16, 'descending stairs': 17, 'vacuum cleaning': 8, 'ironing': 8, 'folding laundry': 4, 'house cleaning': 5, 'playing soccer': 2, 'rope jumping': 6}\n",
      ">>  -24.17870812589971 38.55218330134357\n",
      "<class 'list'> <class 'list'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data_dict = dataReader.generate(unseen_classes=[1, 3], resampling=True, seen_ratio=0.2, unseen_ratio=0.8, window_size=5.21, window_overlap=4.21, resample_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataReader.data[1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in dataReader.data:\n",
    "    print(np.isnan(m).sum(), np.isinf(m).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18252, 36)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['train']['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isinf(data_dict['train']['X']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18252, 36)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['train']['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(data_dict['train']['X']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples :  18252\n",
      "per class count :  {2: 1459, 4: 1844, 5: 760, 6: 1269, 7: 1472, 8: 685, 9: 2483, 10: 443, 11: 841, 12: 770, 13: 1367, 14: 1879, 15: 781, 16: 1451, 17: 377, 18: 371}\n"
     ]
    }
   ],
   "source": [
    "# training dataset\n",
    "train_X, train_y = data_dict['train']['X'], data_dict['train']['y']\n",
    "print(\"number of training samples : \", len(train_y))\n",
    "s = np.unique(train_y, return_counts=True)\n",
    "print(\"per class count : \", dict(zip(s[0], s[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples :  4563\n",
      "per class count :  {2: 348, 4: 499, 5: 189, 6: 336, 7: 370, 8: 146, 9: 594, 10: 96, 11: 240, 12: 183, 13: 342, 14: 462, 15: 195, 16: 391, 17: 81, 18: 91}\n"
     ]
    }
   ],
   "source": [
    "# Seen Evaluation dataset\n",
    "Seval_X, Seval_y = data_dict['eval-seen']['X'], data_dict['eval-seen']['y']\n",
    "print(\"number of training samples : \", len(Seval_y))\n",
    "s = np.unique(Seval_y, return_counts=True)\n",
    "print(\"per class count : \", dict(zip(s[0], s[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples :  3734\n",
      "per class count :  {1: 1880, 3: 1854}\n"
     ]
    }
   ],
   "source": [
    "# Unseen Eval dataset\n",
    "test_X, test_y = data_dict['test']['X'], data_dict['test']['y']\n",
    "print(\"number of training samples : \", len(test_y))\n",
    "s = np.unique(test_y, return_counts=True)\n",
    "print(\"per class count : \", dict(zip(s[0], s[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points :  26549\n",
      "Total number of unseen data :  3734\n",
      "Total number of seen data :  22815\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of data points : \", len(test_y)+len(Seval_y)+len(train_y))\n",
    "print(\"Total number of unseen data : \", len(test_y))\n",
    "print(\"Total number of seen data : \", len(Seval_y)+len(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvts_trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
