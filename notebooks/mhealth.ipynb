{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_log_file(file_name):\n",
    "    parsed_log_lines = []\n",
    "    metric_unit = None\n",
    "    metric_name = None\n",
    "    with open(file_name, \"r\") as log_file:\n",
    "        for line in log_file.readlines():\n",
    "            log_line = line.split()\n",
    "            parsed_log_lines.append(create_dictionary_from_log_line(log_line))\n",
    "\n",
    "            if not metric_name:\n",
    "                metric_name = log_line[0]\n",
    "\n",
    "            if not metric_unit:\n",
    "                metric_unit = log_line[4]\n",
    "\n",
    "    return parsed_log_lines, metric_unit, metric_name \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L:0 > -9.8184\t0.009971\t0.29563\t0.0041863\t0.0041863\t2.1849\t-9.6967\t0.63077\t0.1039\t-0.84053\t-0.68762\t-0.37\t-0.36327\t0.29963\t-8.6499\t-4.5781\t0.18776\t-0.44902\t-1.0103\t0.034483\t-2.35\t-1.6102\t-0.030899\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read sample .log file \n",
    "fpath = '../data/MHEALTH_Dataset/mHealth_subject1.log'\n",
    "with open(fpath, \"r\") as pf:\n",
    "    i = 0\n",
    "    for line in pf.readlines():\n",
    "        print(f\"L:{i} > {line}\")\n",
    "        i+=1\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('../data/MHEALTH_Dataset/mhealth_raw_data.csv')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.1849 -9.6967 0.6307699999999999 0.1039 -0.8405299999999999 -0.68762\n",
      " -8.6499 -4.5781 0.1877599999999999 -0.44902 -1.0103 0.034483] >>  0\n"
     ]
    }
   ],
   "source": [
    "for i, r in data_df.iterrows():\n",
    "    print(r[:12].values, '>> ', r[12])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {\"data\": {}, \"target\": {}, 'collection': []}\n",
    "prev_action = -1\n",
    "starting = True\n",
    "# action_seq = []\n",
    "action_ID = 0\n",
    "for i, s in data_df.iterrows():\n",
    "    # if s[1] != \"0\":\n",
    "    if (prev_action != int(s[12])):\n",
    "        if not(starting):\n",
    "            all_data['data'][action_ID] = np.array(action_seq)\n",
    "            all_data['target'][action_ID] = prev_action\n",
    "            action_ID+=1\n",
    "        action_seq = []\n",
    "    else:\n",
    "        starting = False\n",
    "    # data_seq = np.nan_to_num(np.array(s[3:]), nan=0).astype(np.float16)\n",
    "    # data_seq[np.isnan(data_seq)] = 0\n",
    "    data_seq = s[:12].values\n",
    "    action_seq.append(data_seq)\n",
    "    prev_action = int(s[12])\n",
    "    all_data['collection'].append(data_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 1,\n",
       " 2: 0,\n",
       " 3: 2,\n",
       " 4: 0,\n",
       " 5: 3,\n",
       " 6: 0,\n",
       " 7: 4,\n",
       " 8: 0,\n",
       " 9: 6,\n",
       " 10: 0,\n",
       " 11: 7,\n",
       " 12: 0,\n",
       " 13: 8,\n",
       " 14: 0,\n",
       " 15: 9,\n",
       " 16: 0,\n",
       " 17: 10,\n",
       " 18: 0,\n",
       " 19: 11,\n",
       " 20: 0,\n",
       " 21: 12,\n",
       " 22: 0,\n",
       " 23: 5,\n",
       " 24: 0,\n",
       " 25: 5,\n",
       " 26: 0,\n",
       " 27: 1,\n",
       " 28: 0,\n",
       " 29: 2,\n",
       " 30: 0,\n",
       " 31: 3,\n",
       " 32: 0,\n",
       " 33: 4,\n",
       " 34: 0,\n",
       " 35: 6,\n",
       " 36: 0,\n",
       " 37: 7,\n",
       " 38: 0,\n",
       " 39: 8,\n",
       " 40: 0,\n",
       " 41: 10,\n",
       " 42: 0,\n",
       " 43: 11,\n",
       " 44: 0,\n",
       " 45: 12,\n",
       " 46: 0,\n",
       " 47: 9,\n",
       " 48: 0,\n",
       " 49: 1,\n",
       " 50: 0,\n",
       " 51: 2,\n",
       " 52: 0,\n",
       " 53: 3,\n",
       " 54: 0,\n",
       " 55: 4,\n",
       " 56: 0,\n",
       " 57: 6,\n",
       " 58: 0,\n",
       " 59: 7,\n",
       " 60: 0,\n",
       " 61: 8,\n",
       " 62: 0,\n",
       " 63: 9,\n",
       " 64: 0,\n",
       " 65: 10,\n",
       " 66: 0,\n",
       " 67: 11,\n",
       " 68: 0,\n",
       " 69: 12,\n",
       " 70: 0,\n",
       " 71: 5,\n",
       " 72: 0,\n",
       " 73: 5,\n",
       " 74: 0,\n",
       " 75: 1,\n",
       " 76: 0,\n",
       " 77: 2,\n",
       " 78: 0,\n",
       " 79: 3,\n",
       " 80: 0,\n",
       " 81: 4,\n",
       " 82: 0,\n",
       " 83: 6,\n",
       " 84: 0,\n",
       " 85: 7,\n",
       " 86: 0,\n",
       " 87: 8,\n",
       " 88: 0,\n",
       " 89: 9,\n",
       " 90: 0,\n",
       " 91: 10,\n",
       " 92: 0,\n",
       " 93: 11,\n",
       " 94: 0,\n",
       " 95: 12,\n",
       " 96: 0,\n",
       " 97: 1,\n",
       " 98: 0,\n",
       " 99: 2,\n",
       " 100: 0,\n",
       " 101: 3,\n",
       " 102: 0,\n",
       " 103: 4,\n",
       " 104: 0,\n",
       " 105: 6,\n",
       " 106: 0,\n",
       " 107: 7,\n",
       " 108: 0,\n",
       " 109: 8,\n",
       " 110: 0,\n",
       " 111: 9,\n",
       " 112: 0,\n",
       " 113: 10,\n",
       " 114: 0,\n",
       " 115: 11,\n",
       " 116: 0,\n",
       " 117: 12,\n",
       " 118: 0,\n",
       " 119: 5,\n",
       " 120: 0,\n",
       " 121: 1,\n",
       " 122: 0,\n",
       " 123: 2,\n",
       " 124: 0,\n",
       " 125: 3,\n",
       " 126: 0,\n",
       " 127: 4,\n",
       " 128: 0,\n",
       " 129: 6,\n",
       " 130: 0,\n",
       " 131: 7,\n",
       " 132: 0,\n",
       " 133: 8,\n",
       " 134: 0,\n",
       " 135: 9,\n",
       " 136: 0,\n",
       " 137: 10,\n",
       " 138: 0,\n",
       " 139: 11,\n",
       " 140: 0,\n",
       " 141: 12,\n",
       " 142: 0,\n",
       " 143: 5,\n",
       " 144: 0,\n",
       " 145: 5,\n",
       " 146: 0,\n",
       " 147: 5,\n",
       " 148: 0,\n",
       " 149: 5,\n",
       " 150: 0,\n",
       " 151: 5,\n",
       " 152: 0,\n",
       " 153: 5,\n",
       " 154: 0,\n",
       " 155: 5,\n",
       " 156: 0,\n",
       " 157: 1,\n",
       " 158: 0,\n",
       " 159: 2,\n",
       " 160: 0,\n",
       " 161: 3,\n",
       " 162: 0,\n",
       " 163: 4,\n",
       " 164: 0,\n",
       " 165: 6,\n",
       " 166: 0,\n",
       " 167: 7,\n",
       " 168: 0,\n",
       " 169: 8,\n",
       " 170: 0,\n",
       " 171: 9,\n",
       " 172: 0,\n",
       " 173: 10,\n",
       " 174: 0,\n",
       " 175: 11,\n",
       " 176: 0,\n",
       " 177: 12,\n",
       " 178: 0,\n",
       " 179: 1,\n",
       " 180: 0,\n",
       " 181: 3,\n",
       " 182: 0,\n",
       " 183: 2,\n",
       " 184: 0,\n",
       " 185: 4,\n",
       " 186: 0,\n",
       " 187: 5,\n",
       " 188: 0,\n",
       " 189: 5,\n",
       " 190: 0,\n",
       " 191: 6,\n",
       " 192: 0,\n",
       " 193: 7,\n",
       " 194: 0,\n",
       " 195: 8,\n",
       " 196: 0,\n",
       " 197: 9,\n",
       " 198: 0,\n",
       " 199: 10,\n",
       " 200: 0,\n",
       " 201: 11,\n",
       " 202: 0,\n",
       " 203: 12,\n",
       " 204: 0,\n",
       " 205: 1,\n",
       " 206: 0,\n",
       " 207: 2,\n",
       " 208: 0,\n",
       " 209: 3,\n",
       " 210: 0,\n",
       " 211: 4,\n",
       " 212: 0,\n",
       " 213: 6,\n",
       " 214: 0,\n",
       " 215: 7,\n",
       " 216: 0,\n",
       " 217: 8,\n",
       " 218: 0,\n",
       " 219: 9,\n",
       " 220: 0,\n",
       " 221: 10,\n",
       " 222: 0,\n",
       " 223: 11,\n",
       " 224: 0,\n",
       " 225: 12,\n",
       " 226: 0,\n",
       " 227: 5,\n",
       " 228: 0,\n",
       " 229: 5,\n",
       " 230: 0,\n",
       " 231: 1,\n",
       " 232: 0,\n",
       " 233: 2,\n",
       " 234: 0,\n",
       " 235: 3,\n",
       " 236: 0,\n",
       " 237: 4,\n",
       " 238: 0,\n",
       " 239: 6,\n",
       " 240: 0,\n",
       " 241: 7,\n",
       " 242: 0,\n",
       " 243: 8,\n",
       " 244: 0,\n",
       " 245: 9,\n",
       " 246: 0,\n",
       " 247: 10,\n",
       " 248: 0,\n",
       " 249: 11,\n",
       " 250: 0,\n",
       " 251: 12}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['target']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from scipy.signal import resample\n",
    "from glob import glob \n",
    "import scipy.io \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build MHEALTH dataset data reader\n",
    "class MHEALTHReader(object):\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.readMH()\n",
    "\n",
    "    def readMHFiles(self, data_df):\n",
    "        data, labels, collection = [], [], []\n",
    "\n",
    "        prev_action = -1\n",
    "        starting = True\n",
    "        # action_seq = []\n",
    "        action_ID = 0\n",
    "\n",
    "        for i, s in data_df.iterrows():\n",
    "            if str(s[12]) != \"0\":\n",
    "                if (prev_action != int(s[12])):\n",
    "                    if not(starting):\n",
    "                        data.append(np.array(action_seq))\n",
    "                        labels.append(prev_action)\n",
    "                        action_ID+=1\n",
    "                    action_seq = []\n",
    "                else:\n",
    "                    starting = False\n",
    "\n",
    "                data_seq = s[:12].values\n",
    "                # data_seq[np.isnan(data_seq)] = 0\n",
    "                action_seq.append(data_seq)\n",
    "                prev_action = int(s[12])\n",
    "                # print(prev_action)\n",
    "                collection.append(data_seq)\n",
    "        else: \n",
    "            if len(action_seq) > 1:\n",
    "                data.append(np.array(action_seq))\n",
    "                labels.append(prev_action)\n",
    "        return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n",
    "\n",
    "    def readMH(self):  \n",
    "        label_map = [\n",
    "            #(0, 'other'),\n",
    "            (1, 'Standing still'),\n",
    "            (2, 'Sitting and relaxing'),\n",
    "            (3, 'Lying down'),\n",
    "            (4, 'Walking'),\n",
    "            (5, 'Climbing stairs'),\n",
    "            (6, 'Waist bends forward'),\n",
    "            (7, 'Frontal elevation of arms'), \n",
    "            (8, 'Knees bending (crouching)'),\n",
    "            (9, 'Cycling'),\n",
    "            (10, 'Jogging'),\n",
    "            (11, 'Running'),\n",
    "            (12, 'Jump front & back')\n",
    "        ]\n",
    "        labelToId = {x[0]: i for i, x in enumerate(label_map)}\n",
    "        # print \"label2id=\",labelToId\n",
    "        idToLabel = [x[1] for x in label_map]\n",
    "        # print \"id2label=\",idToLabel\n",
    "        cols = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "        self.cols = cols\n",
    "        self.df = pd.read_csv(self.file_path)\n",
    "        # print \"cols\",cols\n",
    "        self.data, self.targets, self.all_data = self.readMHFiles(self.df)\n",
    "        # self.data = self.data[:, :, cols]\n",
    "        # print(self.data)\n",
    "        # nan_perc = np.isnan(self.data).astype(int).mean()\n",
    "        # print(\"null value percentage \", nan_perc)\n",
    "        # f = lambda x: labelToId[x]\n",
    "        self.targets = np.array([labelToId[i] for i in list(self.targets)])\n",
    "        self.label_map = label_map\n",
    "        self.idToLabel = idToLabel\n",
    "        # return data, idToLabel\n",
    "\n",
    "    def resample(self, signal, freq=10):\n",
    "        step_size = int(100/freq)\n",
    "        seq_len, _ = signal.shape \n",
    "        resample_indx = np.arange(0, seq_len, step_size)\n",
    "        resampled_sig = signal[resample_indx, :]\n",
    "        return resampled_sig\n",
    "\n",
    "    def windowing(self, signal, window_len, overlap):\n",
    "        seq_len = int(window_len*100) # 100Hz compensation \n",
    "        overlap_len = int(overlap*100) # 100Hz\n",
    "        l, _ = signal.shape\n",
    "        if l > seq_len:\n",
    "            windowing_points = np.arange(start=0, stop=l-seq_len, step=seq_len-overlap_len, dtype=int)[:-1]\n",
    "\n",
    "            windows = [signal[p:p+seq_len, :] for p in windowing_points]\n",
    "        else:\n",
    "            windows = []\n",
    "        return windows\n",
    "\n",
    "    def resampling(self, data, targets, window_size, window_overlap, resample_freq):\n",
    "        assert len(data) == len(targets), \"# action data & # action labels are not matching\"\n",
    "        all_data, all_ids, all_labels = [], [], []\n",
    "        for i, d in enumerate(data):\n",
    "            # print(\">>>>>>>>>>>>>>>  \", np.isnan(d).mean())\n",
    "            label = targets[i]\n",
    "            windows = self.windowing(d, window_size, window_overlap)\n",
    "            for w in windows:\n",
    "                # print(np.isnan(w).mean(), label, i)\n",
    "                resample_sig = self.resample(w, resample_freq)\n",
    "                # print(np.isnan(resample_sig).mean(), label, i)\n",
    "                all_data.append(resample_sig)\n",
    "                all_ids.append(i+1)\n",
    "                all_labels.append(label)\n",
    "\n",
    "        return all_data, all_ids, all_labels\n",
    "\n",
    "    def generate(self, unseen_classes, window_size=5.21, window_overlap=1, resample_freq=10, smoothing=False, normalize=False, seen_ratio=0.2, unseen_ratio=0.8):\n",
    "        \n",
    "        def smooth(x, window_len=11, window='hanning'):\n",
    "            if x.ndim != 1:\n",
    "                    raise Exception('smooth only accepts 1 dimension arrays.')\n",
    "            if x.size < window_len:\n",
    "                    raise Exception(\"Input vector needs to be bigger than window size.\")\n",
    "            if window_len<3:\n",
    "                    return x\n",
    "            if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "                    raise Exception(\"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\")\n",
    "            s=np.r_[2*x[0]-x[window_len-1::-1],x,2*x[-1]-x[-1:-window_len:-1]]\n",
    "            if window == 'flat': #moving average\n",
    "                    w=np.ones(window_len,'d')\n",
    "            else:  \n",
    "                    w=eval('np.'+window+'(window_len)')\n",
    "            y=np.convolve(w/w.sum(),s,mode='same')\n",
    "            return y[window_len:-window_len+1]\n",
    "\n",
    "        # assert all([i in list(self.label_map.keys()) for i in unseen_classes]), \"Unknown Class label!\"\n",
    "        seen_classes = [i for i in range(len(self.idToLabel)) if i not in unseen_classes]\n",
    "        unseen_mask = np.in1d(self.targets, unseen_classes)\n",
    "\n",
    "        # build seen dataset \n",
    "        seen_data = self.data[np.invert(unseen_mask)]\n",
    "        seen_targets = self.targets[np.invert(unseen_mask)]\n",
    "\n",
    "        # build unseen dataset\n",
    "        unseen_data = self.data[unseen_mask]\n",
    "        unseen_targets = self.targets[unseen_mask]\n",
    "\n",
    "        # resampling seen and unseen datasets \n",
    "        seen_data, seen_ids, seen_targets = self.resampling(seen_data, seen_targets, window_size, window_overlap, resample_freq)\n",
    "        unseen_data, unseen_ids, unseen_targets = self.resampling(unseen_data, unseen_targets, window_size, window_overlap, resample_freq)\n",
    "\n",
    "        seen_data, seen_targets = np.array(seen_data), np.array(seen_targets)\n",
    "        unseen_data, unseen_targets = np.array(unseen_data), np.array(unseen_targets)\n",
    "\n",
    "        if normalize:\n",
    "            a, b, nft = seen_data.shape \n",
    "            intm_sdata = seen_data.reshape((-1, nft))\n",
    "            intm_udata = unseen_data.reshape((-1, nft))\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            norm_sdata = scaler.fit_transform(intm_sdata)\n",
    "            norm_udata = scaler.transform(intm_udata)\n",
    "\n",
    "            seen_data = norm_sdata.reshape(seen_data.shape)\n",
    "            unseen_data = norm_udata.reshape(unseen_data.shape)\n",
    "\n",
    "        if smoothing:\n",
    "            seen_data = np.apply_along_axis(smooth, axis=1, arr=seen_data)\n",
    "            unseen_data = np.apply_along_axis(smooth, axis=1, arr=unseen_data)\n",
    "        # train-val split\n",
    "        seen_index = list(range(len(seen_targets)))\n",
    "        random.shuffle(seen_index)\n",
    "        split_point = int((1-seen_ratio)*len(seen_index))\n",
    "        fst_index, sec_index = seen_index[:split_point], seen_index[split_point:]\n",
    "        # print(type(fst_index), type(sec_index), type(seen_data), type(seen_targets))\n",
    "        X_seen_train, X_seen_val, y_seen_train, y_seen_val = seen_data[fst_index,:], seen_data[sec_index,:], seen_targets[fst_index], seen_targets[sec_index]\n",
    "        \n",
    "\n",
    "        data = {'train': {\n",
    "                        'X': X_seen_train,\n",
    "                        'y': y_seen_train\n",
    "                        },\n",
    "                'eval-seen':{\n",
    "                        'X': X_seen_val,\n",
    "                        'y': y_seen_val\n",
    "                        },\n",
    "                'test': {\n",
    "                        'X': unseen_data,\n",
    "                        'y': unseen_targets\n",
    "                        },\n",
    "                'seen_classes': seen_classes,\n",
    "                'unseen_classes': unseen_classes\n",
    "                }\n",
    "\n",
    "        return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deela\\AppData\\Local\\Temp\\ipykernel_17504\\923409940.py:36: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n"
     ]
    }
   ],
   "source": [
    "dataReader = MHEALTHReader('../data/MHEALTH_Dataset/mhealth_raw_data.csv')\n",
    "actionList = dataReader.idToLabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dataReader.generate(unseen_classes=[1, 3], seen_ratio=0.2, unseen_ratio=0.8, window_size=5.21, window_overlap=4.21, resample_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Standing still',\n",
       " 'Sitting and relaxing',\n",
       " 'Lying down',\n",
       " 'Walking',\n",
       " 'Climbing stairs',\n",
       " 'Waist bends forward',\n",
       " 'Frontal elevation of arms',\n",
       " 'Knees bending (crouching)',\n",
       " 'Cycling',\n",
       " 'Jogging',\n",
       " 'Running',\n",
       " 'Jump front & back']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataReader.idToLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples :  1819\n",
      "per class count :  {0: 205, 2: 199, 4: 226, 5: 179, 6: 181, 7: 185, 8: 202, 9: 205, 10: 195, 11: 42}\n"
     ]
    }
   ],
   "source": [
    "# training dataset\n",
    "train_X, train_y = data_dict['train']['X'], data_dict['train']['y']\n",
    "print(\"number of training samples : \", len(train_y))\n",
    "s = np.unique(train_y, return_counts=True)\n",
    "print(\"per class count : \", dict(zip(s[0], s[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples :  455\n",
      "per class count :  {0: 45, 2: 51, 4: 48, 5: 48, 6: 56, 7: 51, 8: 48, 9: 45, 10: 55, 11: 8}\n"
     ]
    }
   ],
   "source": [
    "# Seen Evaluation dataset\n",
    "Seval_X, Seval_y = data_dict['eval-seen']['X'], data_dict['eval-seen']['y']\n",
    "print(\"number of training samples : \", len(Seval_y))\n",
    "s = np.unique(Seval_y, return_counts=True)\n",
    "print(\"per class count : \", dict(zip(s[0], s[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples :  500\n",
      "per class count :  {1: 250, 3: 250}\n"
     ]
    }
   ],
   "source": [
    "# Unseen Eval dataset\n",
    "test_X, test_y = data_dict['test']['X'], data_dict['test']['y']\n",
    "print(\"number of training samples : \", len(test_y))\n",
    "s = np.unique(test_y, return_counts=True)\n",
    "print(\"per class count : \", dict(zip(s[0], s[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points :  2774\n",
      "Total number of unseen data :  500\n",
      "Total number of seen data :  2274\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of data points : \", len(test_y)+len(Seval_y)+len(train_y))\n",
    "print(\"Total number of unseen data : \", len(test_y))\n",
    "print(\"Total number of seen data : \", len(Seval_y)+len(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvts_trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
