{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import simplejson as json\n",
    "import sqlite3\n",
    "import copy\n",
    "from numpy import nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [56], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m sample_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../data/PAMAP2_Dataset/Protocol/subject101.dat\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      3\u001b[0m datContent \u001b[39m=\u001b[39m [i\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mopen\u001b[39m(sample_file)\u001b[39m.\u001b[39mreadlines()]\n\u001b[1;32m----> 4\u001b[0m datDF \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(datContent)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "sample_file = '../data/PAMAP2_Dataset/Protocol/subject101.dat'\n",
    "\n",
    "datContent = [i.strip().split() for i in open(sample_file).readlines()]\n",
    "datDF = pd.DataFrame(datContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {\"data\": {}, \"target\": {}}\n",
    "prev_action = -1\n",
    "starting = True\n",
    "# action_seq = []\n",
    "action_ID = 0\n",
    "for l in open(sample_file).readlines():\n",
    "    s = l.strip().split()\n",
    "    if (prev_action != int(s[1])):\n",
    "        if not(starting):\n",
    "            all_data['data'][action_ID] = np.array(action_seq)\n",
    "            all_data['target'][action_ID] = prev_action\n",
    "            action_ID+=1\n",
    "        action_seq = []\n",
    "    else:\n",
    "        starting = False\n",
    "    data_seq = np.nan_to_num(np.array(s[3:]), nan=0)\n",
    "    # data_seq[np.isnan(data_seq)] = 0\n",
    "    action_seq.append(data_seq)\n",
    "    prev_action = int(s[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(file_path):\n",
    "        all_data = {\"data\": {}, \"target\": {}, 'collection': []}\n",
    "        prev_action = -1\n",
    "        starting = True\n",
    "        # action_seq = []\n",
    "        action_ID = 0\n",
    "        for l in open(file_path).readlines():\n",
    "            s = l.strip().split()\n",
    "            if s[1] != \"0\":\n",
    "                if (prev_action != int(s[1])):\n",
    "                    if not(starting):\n",
    "                        all_data['data'][action_ID] = np.array(action_seq)\n",
    "                        all_data['target'][action_ID] = prev_action\n",
    "                        action_ID+=1\n",
    "                    action_seq = []\n",
    "                else:\n",
    "                    starting = False\n",
    "                data_seq = np.nan_to_num(np.array(s[3:]), nan=0).astype(np.float16)\n",
    "                # data_seq[np.isnan(data_seq)] = 0\n",
    "                action_seq.append(data_seq)\n",
    "                prev_action = int(s[1])\n",
    "                all_data['collection'].append(data_seq)\n",
    "        return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = '../data/PAMAP2_Dataset/Protocol/subject101.dat'\n",
    "file_data = readFile(sample_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003709971448422462"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(np.array(file_data['collection'])).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(file_data['collection'])\n",
    "intep_df = df.interpolate(method='linear', limit_direction='backward', axis=0)\n",
    "intep_data = intep_df.values \n",
    "np.isnan(intep_data).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249957, 51)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intep_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23480, 51)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['data'][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.38</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>30</td>\n",
       "      <td>2.37223</td>\n",
       "      <td>8.60074</td>\n",
       "      <td>3.51048</td>\n",
       "      <td>2.43954</td>\n",
       "      <td>8.76165</td>\n",
       "      <td>3.35465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00830026</td>\n",
       "      <td>0.00925038</td>\n",
       "      <td>-0.0175803</td>\n",
       "      <td>-61.1888</td>\n",
       "      <td>-38.9599</td>\n",
       "      <td>-58.1438</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.39</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>2.18837</td>\n",
       "      <td>8.5656</td>\n",
       "      <td>3.66179</td>\n",
       "      <td>2.39494</td>\n",
       "      <td>8.55081</td>\n",
       "      <td>3.64207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00657665</td>\n",
       "      <td>-0.00463778</td>\n",
       "      <td>0.00036825</td>\n",
       "      <td>-59.8479</td>\n",
       "      <td>-38.8919</td>\n",
       "      <td>-58.5253</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>2.37357</td>\n",
       "      <td>8.60107</td>\n",
       "      <td>3.54898</td>\n",
       "      <td>2.30514</td>\n",
       "      <td>8.53644</td>\n",
       "      <td>3.7328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00301426</td>\n",
       "      <td>0.000148236</td>\n",
       "      <td>0.022495</td>\n",
       "      <td>-60.7361</td>\n",
       "      <td>-39.4138</td>\n",
       "      <td>-58.3999</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.41</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>2.07473</td>\n",
       "      <td>8.52853</td>\n",
       "      <td>3.66021</td>\n",
       "      <td>2.33528</td>\n",
       "      <td>8.53622</td>\n",
       "      <td>3.73277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00317498</td>\n",
       "      <td>-0.0203009</td>\n",
       "      <td>0.0112754</td>\n",
       "      <td>-60.4091</td>\n",
       "      <td>-38.7635</td>\n",
       "      <td>-58.3956</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.42</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>2.22936</td>\n",
       "      <td>8.83122</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2.23055</td>\n",
       "      <td>8.59741</td>\n",
       "      <td>3.76295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0126977</td>\n",
       "      <td>-0.0143027</td>\n",
       "      <td>-0.00282262</td>\n",
       "      <td>-61.5199</td>\n",
       "      <td>-39.3879</td>\n",
       "      <td>-58.2694</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1    2   3        4        5        6        7        8        9   \\\n",
       "0  8.38  0  104  30  2.37223  8.60074  3.51048  2.43954  8.76165  3.35465   \n",
       "1  8.39  0  NaN  30  2.18837   8.5656  3.66179  2.39494  8.55081  3.64207   \n",
       "2   8.4  0  NaN  30  2.37357  8.60107  3.54898  2.30514  8.53644   3.7328   \n",
       "3  8.41  0  NaN  30  2.07473  8.52853  3.66021  2.33528  8.53622  3.73277   \n",
       "4  8.42  0  NaN  30  2.22936  8.83122      3.7  2.23055  8.59741  3.76295   \n",
       "\n",
       "   ...           44           45           46        47        48        49  \\\n",
       "0  ...   0.00830026   0.00925038   -0.0175803  -61.1888  -38.9599  -58.1438   \n",
       "1  ...  -0.00657665  -0.00463778   0.00036825  -59.8479  -38.8919  -58.5253   \n",
       "2  ...   0.00301426  0.000148236     0.022495  -60.7361  -39.4138  -58.3999   \n",
       "3  ...   0.00317498   -0.0203009    0.0112754  -60.4091  -38.7635  -58.3956   \n",
       "4  ...    0.0126977   -0.0143027  -0.00282262  -61.5199  -39.3879  -58.2694   \n",
       "\n",
       "  50 51 52 53  \n",
       "0  1  0  0  0  \n",
       "1  1  0  0  0  \n",
       "2  1  0  0  0  \n",
       "3  1  0  0  0  \n",
       "4  1  0  0  0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=54, step=1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "datDF.loc[datDF[1]=='0'].to_csv('../tmp/pamap2_act1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(file_path):\n",
    "    all_data = {\"data\": {}, \"target\": {}, \"collection\": []}\n",
    "    prev_action = -1\n",
    "    starting = True\n",
    "    # action_seq = []\n",
    "    action_ID = 0\n",
    "    for l in open(file_path).readlines():\n",
    "        s = l.strip().split()\n",
    "        if (prev_action != int(s[1])):\n",
    "            if not(starting):\n",
    "                all_data['data'][action_ID] = np.array(action_seq)\n",
    "                all_data['target'][action_ID] = prev_action\n",
    "                action_ID+=1\n",
    "            action_seq = []\n",
    "        else:\n",
    "            starting = False\n",
    "        data_seq = np.nan_to_num(np.array(s[3:]), nan=0)\n",
    "        # data_seq[np.isnan(data_seq)] = 0\n",
    "        action_seq.append(data_seq)\n",
    "        prev_action = int(s[1])\n",
    "        all_data['collection'].append(data_seq)\n",
    "    return all_data\n",
    "\n",
    "def readPamap2Files(data_path, filelist, cols, labelToId):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i, filename in enumerate(filelist):\n",
    "        print('Reading file %d of %d' % (i+1, len(filelist)))\n",
    "        fpath = os.path.join(data_path, filename)\n",
    "        file_data = readFile(fpath)\n",
    "        data.extend(list(file_data['data'].values()))\n",
    "        labels.extend(list(file_data['target'].values()))\n",
    "    \n",
    "    return {'inputs': np.asarray(data), 'targets': np.asarray(labels, dtype=int)+1}\n",
    "\n",
    "def readPamap2(data_path):\n",
    "    files = {\n",
    "        'train': ['subject101.dat', 'subject102.dat','subject103.dat','subject104.dat', 'subject107.dat', 'subject108.dat', 'subject109.dat'],\n",
    "        'test': ['subject106.dat']\n",
    "    }\n",
    "    label_map = [\n",
    "        # (0, 'other'),\n",
    "        (1, 'lying'),\n",
    "        (2, 'sitting'),\n",
    "        (3, 'standing'),\n",
    "        (4, 'walking'),\n",
    "        (5, 'running'),\n",
    "        (6, 'cycling'),\n",
    "        (7, 'Nordic walking'),\n",
    "        (9, 'watching TV'),\n",
    "        (10, 'computer work'),\n",
    "        (11, 'car driving'),\n",
    "        (12, 'ascending stairs'),\n",
    "        (13, 'descending stairs'),\n",
    "        (16, 'vacuum cleaning'),\n",
    "        (17, 'ironing'),\n",
    "        (18, 'folding laundry'),\n",
    "        (19, 'house cleaning'),\n",
    "        (20, 'playing soccer'),\n",
    "        (24, 'rope jumping')\n",
    "    ]\n",
    "    labelToId = {str(x[0]): i for i, x in enumerate(label_map)}\n",
    "    # print \"label2id=\",labelToId\n",
    "    idToLabel = [x[1] for x in label_map]\n",
    "    # print \"id2label=\",idToLabel\n",
    "    cols = [\n",
    "            1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
    "            35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53\n",
    "            ]\n",
    "    # print \"cols\",cols\n",
    "    data = {dataset: readPamap2Files(data_path, files[dataset], cols, labelToId)\n",
    "            for dataset in ('train', 'test')}\n",
    "    return data, idToLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 1 of 7\n",
      "Reading file 2 of 7\n",
      "Reading file 3 of 7\n",
      "Reading file 4 of 7\n",
      "Reading file 5 of 7\n",
      "Reading file 6 of 7\n",
      "Reading file 7 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deela\\AppData\\Local\\Temp\\ipykernel_29436\\2408505951.py:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return {'inputs': np.asarray(data), 'targets': np.asarray(labels, dtype=int)+1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 1 of 1\n"
     ]
    }
   ],
   "source": [
    "pam_data = readPamap2('../data/PAMAP2_Dataset/Protocol/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pam_data[0]['train']['inputs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lying',\n",
       " 'sitting',\n",
       " 'standing',\n",
       " 'walking',\n",
       " 'running',\n",
       " 'cycling',\n",
       " 'Nordic walking',\n",
       " 'watching TV',\n",
       " 'computer work',\n",
       " 'car driving',\n",
       " 'ascending stairs',\n",
       " 'descending stairs',\n",
       " 'vacuum cleaning',\n",
       " 'ironing',\n",
       " 'folding laundry',\n",
       " 'house cleaning',\n",
       " 'playing soccer',\n",
       " 'rope jumping']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pam_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pam_data[0]['train']\n",
    "test_data = pam_data[0]['test']\n",
    "# num_i, num_c = train_data['inputs'].shape \n",
    "classes = pam_data[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  1, 18,  1, 17,  1, 13,  1, 14,  1, 13, 14,  1,  5,\n",
       "        1,  8,  1,  7,  1,  6,  1, 25,  1,  2,  1,  3,  1,  4,  1, 18,  1,\n",
       "       17,  1, 13,  1, 14,  1, 13, 14,  1,  5,  1,  8,  1,  7,  1,  6,  1,\n",
       "       25,  1,  2,  1,  3,  4, 18,  1, 17,  1, 14,  1, 13, 14,  1, 13, 14,\n",
       "        1,  5,  1,  2,  1,  3,  4, 18,  1, 17,  1, 13,  1, 14,  1, 13, 14,\n",
       "        1,  5,  1,  8,  1,  7,  1,  1,  2,  1,  3,  1,  4, 18,  1, 17,  1,\n",
       "       13,  1, 14,  1, 13, 14,  1,  5,  1,  8,  1,  7,  1,  1,  2,  1,  3,\n",
       "        4, 18,  1, 17,  1, 13,  1, 14,  1, 13, 14,  1,  5,  1,  7,  1,  8,\n",
       "        1,  6,  1, 25,  1, 25])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vals, train_labels = train_data['inputs'], train_data['targets']\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False,  True,  True,  True,  True, False,  True,\n",
       "       False,  True, False,  True, False, False,  True, False,  True,\n",
       "       False,  True, False,  True,  True,  True, False,  True, False,\n",
       "        True, False,  True,  True,  True,  True,  True, False,  True,\n",
       "       False,  True, False,  True, False, False,  True, False,  True,\n",
       "       False,  True, False,  True,  True,  True, False,  True, False,\n",
       "        True, False,  True,  True,  True, False,  True, False,  True,\n",
       "       False, False,  True, False, False,  True, False,  True, False,\n",
       "        True, False,  True,  True,  True, False,  True, False,  True,\n",
       "       False,  True, False, False,  True, False,  True, False,  True,\n",
       "       False,  True,  True, False,  True, False,  True,  True,  True,\n",
       "        True, False,  True, False,  True, False,  True, False, False,\n",
       "        True, False,  True, False,  True, False,  True,  True, False,\n",
       "        True, False,  True,  True,  True, False,  True, False,  True,\n",
       "       False,  True, False, False,  True, False,  True, False,  True,\n",
       "       False,  True,  True,  True, False,  True, False])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_classes = [1, 4, 6, 18]\n",
    "flt_mask = np.in1d(train_labels, filtered_classes)\n",
    "flt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True, False, False, False, False,  True, False,\n",
       "        True, False,  True, False,  True,  True, False,  True, False,\n",
       "        True, False,  True, False, False, False,  True, False,  True,\n",
       "       False,  True, False, False, False, False, False,  True, False,\n",
       "        True, False,  True, False,  True,  True, False,  True, False,\n",
       "        True, False,  True, False, False, False,  True, False,  True,\n",
       "       False,  True, False, False, False,  True, False,  True, False,\n",
       "        True,  True, False,  True,  True, False,  True, False,  True,\n",
       "       False,  True, False, False, False,  True, False,  True, False,\n",
       "        True, False,  True,  True, False,  True, False,  True, False,\n",
       "        True, False, False,  True, False,  True, False, False, False,\n",
       "       False,  True, False,  True, False,  True, False,  True,  True,\n",
       "       False,  True, False,  True, False,  True, False, False,  True,\n",
       "       False,  True, False, False, False,  True, False,  True, False,\n",
       "        True, False,  True,  True, False,  True, False,  True, False,\n",
       "        True, False, False, False,  True, False,  True])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.invert(flt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28,) (28,)\n"
     ]
    }
   ],
   "source": [
    "flt_label = train_labels[flt_mask]\n",
    "flt_data = train_vals[flt_mask]\n",
    "print(flt_data.shape, flt_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build sample ZSL training dataloader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from scipy.signal import resample\n",
    "\n",
    "class PAMAP2Reader(object):\n",
    "    def __init__(self, root_path):\n",
    "        self.root_path = root_path\n",
    "        self.readPamap2()\n",
    "\n",
    "    def readFile(self, file_path):\n",
    "        all_data = {\"data\": {}, \"target\": {}, 'collection': []}\n",
    "        prev_action = -1\n",
    "        starting = True\n",
    "        # action_seq = []\n",
    "        action_ID = 0\n",
    "        for l in open(file_path).readlines():\n",
    "            s = l.strip().split()\n",
    "            # if s[1] != \"0\":\n",
    "            if (prev_action != int(s[1])):\n",
    "                if not(starting):\n",
    "                    all_data['data'][action_ID] = np.array(action_seq)\n",
    "                    all_data['target'][action_ID] = prev_action\n",
    "                    action_ID+=1\n",
    "                action_seq = []\n",
    "            else:\n",
    "                starting = False\n",
    "            data_seq = np.nan_to_num(np.array(s[3:]), nan=0).astype(np.float16)\n",
    "            # data_seq[np.isnan(data_seq)] = 0\n",
    "            action_seq.append(data_seq)\n",
    "            prev_action = int(s[1])\n",
    "            all_data['collection'].append(data_seq)\n",
    "        return all_data\n",
    "\n",
    "    def readPamap2Files(self, filelist, cols, labelToId):\n",
    "        data = []\n",
    "        labels = []\n",
    "        for i, filename in enumerate(filelist):\n",
    "            print('Reading file %d of %d' % (i+1, len(filelist)))\n",
    "            fpath = os.path.join(self.root_path, filename)\n",
    "            file_data = self.readFile(fpath)\n",
    "            data.extend(list(file_data['data'].values()))\n",
    "            labels.extend(list(file_data['target'].values()))\n",
    "        \n",
    "        return np.asarray(data), np.asarray(labels, dtype=int)\n",
    "\n",
    "    def readPamap2(self):\n",
    "        files = ['subject101.dat', 'subject102.dat','subject103.dat','subject104.dat', 'subject105.dat', 'subject106.dat', 'subject107.dat', 'subject108.dat', 'subject109.dat']#['subject101.dat', 'subject102.dat'] #['subject101.dat', 'subject102.dat','subject103.dat','subject104.dat', 'subject105.dat', 'subject106.dat', 'subject107.dat', 'subject108.dat', 'subject109.dat']\n",
    "            \n",
    "        label_map = [\n",
    "            (0, 'other'),\n",
    "            (1, 'lying'),\n",
    "            (2, 'sitting'),\n",
    "            (3, 'standing'),\n",
    "            (4, 'walking'),\n",
    "            (5, 'running'),\n",
    "            (6, 'cycling'),\n",
    "            (7, 'Nordic walking'),\n",
    "            (9, 'watching TV'),\n",
    "            (10, 'computer work'),\n",
    "            (11, 'car driving'),\n",
    "            (12, 'ascending stairs'),\n",
    "            (13, 'descending stairs'),\n",
    "            (16, 'vacuum cleaning'),\n",
    "            (17, 'ironing'),\n",
    "            (18, 'folding laundry'),\n",
    "            (19, 'house cleaning'),\n",
    "            (20, 'playing soccer'),\n",
    "            (24, 'rope jumping')\n",
    "        ]\n",
    "        labelToId = {x[0]: i for i, x in enumerate(label_map)}\n",
    "        # print \"label2id=\",labelToId\n",
    "        idToLabel = [x[1] for x in label_map]\n",
    "        # print \"id2label=\",idToLabel\n",
    "        cols = [\n",
    "                1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
    "                35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53\n",
    "                ]\n",
    "        # print \"cols\",cols\n",
    "        self.data, self.targets = self.readPamap2Files(files, cols, labelToId)\n",
    "        f = lambda x: labelToId[x]\n",
    "        print(\"labelToId > \", labelToId)\n",
    "        self.targets = np.array([labelToId[i] for i in list(self.targets)]) #np.vectorize(labelToId.get)(self.targets)\n",
    "        self.label_map = label_map\n",
    "        self.idToLabel = idToLabel\n",
    "        # return data, idToLabel\n",
    "\n",
    "    def resample(self, signal, freq=10):\n",
    "        step_size = int(100/10)\n",
    "        seq_len, _ = signal.shape \n",
    "        resample_indx = np.arange(0, seq_len, step_size)\n",
    "        resampled_sig = signal[resample_indx, :]\n",
    "        return resampled_sig\n",
    "\n",
    "    def windowing(self, signal, window_len, overlap):\n",
    "        seq_len = int(window_len*100) # 100Hz compensation \n",
    "        overlap_len = int(overlap*100) # 100Hz\n",
    "        l, _ = signal.shape\n",
    "        windowing_points = np.arange(start=0, stop=l, step=seq_len, dtype=int)[:-1]\n",
    "        windowing_points = windowing_points-overlap_len\n",
    "        windowing_points[0] = 0 \n",
    "\n",
    "        windows = [signal[p:p+seq_len, :] for p in windowing_points]\n",
    "        return windows\n",
    "\n",
    "    def resampling(self, data, targets, window_size, window_overlap, resample_freq):\n",
    "        assert len(data) == len(targets), \"# action data & # action labels are not matching\"\n",
    "        all_data, all_ids, all_labels = [], [], []\n",
    "        for i, d in enumerate(data):\n",
    "            label = targets[i]\n",
    "            windows = self.windowing(d, window_size, window_overlap)\n",
    "            for w in windows:\n",
    "                resample_sig = self.resample(w, resample_freq)\n",
    "                all_data.append(resample_sig)\n",
    "                all_ids.append(i+1)\n",
    "                all_labels.append(label)\n",
    "\n",
    "        return all_data, all_ids, all_labels\n",
    "\n",
    "    def generate(self, unseen_classes, resampling=True, window_size=5.21, window_overlap=1, resample_freq=10, seen_ratio=0.2, unseen_ratio=0.8):\n",
    "        # assert all([i in list(self.label_map.keys()) for i in unseen_classes]), \"Unknown Class label!\"\n",
    "        seen_classes = [i for i in self.idToLabel if i not in unseen_classes]\n",
    "        unseen_mask = np.in1d(self.targets, unseen_classes)\n",
    "\n",
    "        # build seen dataset \n",
    "        seen_data = self.data[np.invert(unseen_mask)]\n",
    "        seen_targets = self.targets[np.invert(unseen_mask)]\n",
    "\n",
    "        # build unseen dataset\n",
    "        unseen_data = self.data[unseen_mask]\n",
    "        unseen_targets = self.targets[unseen_mask]\n",
    "\n",
    "        # resampling seen and unseen datasets \n",
    "        seen_data, seen_ids, seen_targets = self.resampling(seen_data, seen_targets, window_size, window_overlap, resample_freq)\n",
    "        unseen_data, unseen_ids, unseen_targets = self.resampling(unseen_data, unseen_targets, window_size, window_overlap, resample_freq)\n",
    "\n",
    "        seen_data, seen_targets = np.array(seen_data), np.array(seen_targets)\n",
    "        unseen_data, unseen_targets = np.array(unseen_data), np.array(unseen_targets)\n",
    "        # train-val split\n",
    "        seen_index = list(range(len(seen_targets)))\n",
    "        random.shuffle(seen_index)\n",
    "        split_point = int((1-seen_ratio)*len(seen_index))\n",
    "        fst_index, sec_index = seen_index[:split_point], seen_index[split_point:]\n",
    "        print(type(fst_index), type(sec_index), type(seen_data), type(seen_targets))\n",
    "        X_seen_train, X_seen_val, y_seen_train, y_seen_val = seen_data[fst_index,:], seen_data[sec_index,:], seen_targets[fst_index], seen_targets[sec_index]\n",
    "        \n",
    "        # val-test split\n",
    "        unseen_index = list(range(len(unseen_targets)))\n",
    "        random.shuffle(unseen_index)\n",
    "        split_point = int((1-unseen_ratio)*len(unseen_index))\n",
    "        fst_index, sec_index = unseen_index[:split_point], unseen_index[split_point:]\n",
    "\n",
    "        X_unseen_val, X_unseen_test, y_unseen_val, y_unseen_test = unseen_data[fst_index,:], unseen_data[sec_index,:], unseen_targets[fst_index], unseen_targets[sec_index]\n",
    "\n",
    "        data = {'train': {\n",
    "                        'X': X_seen_train,\n",
    "                        'y': y_seen_train\n",
    "                        },\n",
    "                'eval-seen':{\n",
    "                        'X': X_seen_val,\n",
    "                        'y': y_seen_val\n",
    "                        },\n",
    "                'eval-unseen':{\n",
    "                        'X': X_unseen_val,\n",
    "                        'y': y_unseen_val\n",
    "                        },\n",
    "                'test': {\n",
    "                        'X': X_unseen_test,\n",
    "                        'y': y_unseen_test\n",
    "                        }\n",
    "                }\n",
    "\n",
    "        return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 1 of 9\n",
      "Reading file 2 of 9\n",
      "Reading file 3 of 9\n",
      "Reading file 4 of 9\n",
      "Reading file 5 of 9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataReader \u001b[39m=\u001b[39m PAMAP2Reader(\u001b[39m'\u001b[39;49m\u001b[39m../data/PAMAP2_Dataset/Protocol/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn [5], line 10\u001b[0m, in \u001b[0;36mPAMAP2Reader.__init__\u001b[1;34m(self, root_path)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, root_path):\n\u001b[0;32m      9\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_path \u001b[39m=\u001b[39m root_path\n\u001b[1;32m---> 10\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadPamap2()\n",
      "Cell \u001b[1;32mIn [5], line 81\u001b[0m, in \u001b[0;36mPAMAP2Reader.readPamap2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m cols \u001b[39m=\u001b[39m [\n\u001b[0;32m     77\u001b[0m         \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m6\u001b[39m, \u001b[39m7\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m9\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m11\u001b[39m, \u001b[39m12\u001b[39m, \u001b[39m13\u001b[39m, \u001b[39m14\u001b[39m, \u001b[39m15\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m17\u001b[39m, \u001b[39m18\u001b[39m, \u001b[39m19\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m21\u001b[39m, \u001b[39m22\u001b[39m, \u001b[39m23\u001b[39m, \u001b[39m24\u001b[39m, \u001b[39m25\u001b[39m, \u001b[39m26\u001b[39m, \u001b[39m27\u001b[39m, \u001b[39m28\u001b[39m, \u001b[39m29\u001b[39m, \u001b[39m30\u001b[39m, \u001b[39m31\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m33\u001b[39m, \u001b[39m34\u001b[39m,\n\u001b[0;32m     78\u001b[0m         \u001b[39m35\u001b[39m, \u001b[39m36\u001b[39m, \u001b[39m37\u001b[39m, \u001b[39m38\u001b[39m, \u001b[39m39\u001b[39m, \u001b[39m40\u001b[39m, \u001b[39m41\u001b[39m, \u001b[39m42\u001b[39m, \u001b[39m43\u001b[39m, \u001b[39m44\u001b[39m, \u001b[39m45\u001b[39m, \u001b[39m46\u001b[39m, \u001b[39m47\u001b[39m, \u001b[39m48\u001b[39m, \u001b[39m49\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39m51\u001b[39m, \u001b[39m52\u001b[39m, \u001b[39m53\u001b[39m\n\u001b[0;32m     79\u001b[0m         ]\n\u001b[0;32m     80\u001b[0m \u001b[39m# print \"cols\",cols\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadPamap2Files(files, cols, labelToId)\n\u001b[0;32m     82\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: labelToId[x]\n\u001b[0;32m     83\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlabelToId > \u001b[39m\u001b[39m\"\u001b[39m, labelToId)\n",
      "Cell \u001b[1;32mIn [5], line 42\u001b[0m, in \u001b[0;36mPAMAP2Reader.readPamap2Files\u001b[1;34m(self, filelist, cols, labelToId)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mReading file \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(filelist)))\n\u001b[0;32m     41\u001b[0m fpath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_path, filename)\n\u001b[1;32m---> 42\u001b[0m file_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadFile(fpath)\n\u001b[0;32m     43\u001b[0m data\u001b[39m.\u001b[39mextend(\u001b[39mlist\u001b[39m(file_data[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues()))\n\u001b[0;32m     44\u001b[0m labels\u001b[39m.\u001b[39mextend(\u001b[39mlist\u001b[39m(file_data[\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues()))\n",
      "Cell \u001b[1;32mIn [5], line 29\u001b[0m, in \u001b[0;36mPAMAP2Reader.readFile\u001b[1;34m(self, file_path)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     starting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m data_seq \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mnan_to_num(np\u001b[39m.\u001b[39;49marray(s[\u001b[39m3\u001b[39;49m:]), nan\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat16)\n\u001b[0;32m     30\u001b[0m \u001b[39m# data_seq[np.isnan(data_seq)] = 0\u001b[39;00m\n\u001b[0;32m     31\u001b[0m action_seq\u001b[39m.\u001b[39mappend(data_seq)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mnan_to_num\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\numpy\\lib\\type_check.py:498\u001b[0m, in \u001b[0;36mnan_to_num\u001b[1;34m(x, copy, nan, posinf, neginf)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_nan_to_num_dispatcher)\n\u001b[0;32m    405\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnan_to_num\u001b[39m(x, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, nan\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, posinf\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, neginf\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    406\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \u001b[39m    Replace NaN with zero and infinity with large finite numbers (default\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39m    behaviour) or with the numbers defined by the user using the `nan`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[39m    array([222222.+111111.j, 111111.     +0.j, 111111.+222222.j])\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 498\u001b[0m     x \u001b[39m=\u001b[39m _nx\u001b[39m.\u001b[39;49marray(x, subok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m    499\u001b[0m     xtype \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype\n\u001b[0;32m    501\u001b[0m     isscalar \u001b[39m=\u001b[39m (x\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataReader = PAMAP2Reader('../data/PAMAP2_Dataset/Protocol/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2, 13, 12, 10, 11, 10, 11,  3,  6,  5,  4,  0,  1,  2, 13,\n",
       "       12, 10, 11, 10, 11,  3,  6,  5,  4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataReader.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(27187, 51),\n",
       " (23480, 51),\n",
       " (21717, 51),\n",
       " (23573, 51),\n",
       " (22941, 51),\n",
       " (8120, 51),\n",
       " (7480, 51),\n",
       " (7770, 51),\n",
       " (7419, 51),\n",
       " (22253, 51),\n",
       " (20265, 51),\n",
       " (23575, 51),\n",
       " (21265, 51),\n",
       " (23430, 51),\n",
       " (22345, 51),\n",
       " (25576, 51),\n",
       " (28880, 51),\n",
       " (20683, 51),\n",
       " (8725, 51),\n",
       " (7791, 51),\n",
       " (8617, 51),\n",
       " (7422, 51),\n",
       " (32533, 51),\n",
       " (29739, 51),\n",
       " (25108, 51),\n",
       " (9238, 51)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.shape for i in dataReader.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8120, 51)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_seq = dataReader.data[5]\n",
    "sample_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(signal, freq=10):\n",
    "    step_size = int(100/10)\n",
    "    seq_len, _ = signal.shape \n",
    "    resample_indx = np.arange(0, seq_len, step_size)\n",
    "    resampled_sig = signal[resample_indx, :]\n",
    "    return resampled_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(812, 51)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_signal = resample(sample_seq, freq=10)\n",
    "res_signal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(res_signal).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowing(signal, window_len, overlap):\n",
    "    seq_len = int(window_len*100) # 100Hz compensation \n",
    "    overlap_len = int(overlap*100) # 100Hz\n",
    "    l, _ = signal.shape\n",
    "    windowing_points = np.arange(start=0, stop=l, step=seq_len, dtype=int)[:-1]\n",
    "    windowing_points = windowing_points-overlap_len\n",
    "    windowing_points[0] = 0 \n",
    "    print(windowing_points.dtype)\n",
    "    windows = np.array([signal[p:p+seq_len, :] for p in windowing_points])\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows = windowing(sample_seq, 5.21, 1)\n",
    "len(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data_dict = dataReader.generate(unseen_classes=[2, 4, 14, 18], resampling=True, seen_ratio=0.2, unseen_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620, 53, 51)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['train']['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(388,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['train']['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lying',\n",
       " 'sitting',\n",
       " 'standing',\n",
       " 'walking',\n",
       " 'running',\n",
       " 'cycling',\n",
       " 'Nordic walking',\n",
       " 'watching TV',\n",
       " 'computer work',\n",
       " 'car driving',\n",
       " 'ascending stairs',\n",
       " 'descending stairs',\n",
       " 'vacuum cleaning',\n",
       " 'ironing',\n",
       " 'folding laundry',\n",
       " 'house cleaning',\n",
       " 'playing soccer',\n",
       " 'rope jumping']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataReader.idToLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> 190\n"
     ]
    }
   ],
   "source": [
    "print(type(dataReader.data), len(dataReader.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.shape for i in dataReader.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divmod(12, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0, 1200, 2400, 3600, 4800, 6000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(start=0, stop=6391, step=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplest sampling strategy \n",
    "# frequency : 10Hz (human-eye sensitivity) \n",
    "# sequence length : 120 points\n",
    "# expected minimum length : 1200\n",
    "# window sampling method : shifting window method with 200 point overlapping\n",
    "from scipy.signal import resample\n",
    "\n",
    "def signal_sampling(data, labels):\n",
    "    assert len(data) == len(labels), \"# action data & # action labels are not matching\"\n",
    "    all_data, all_ids, all_labels = [], [], []\n",
    "    for i, d in enumerate(data):\n",
    "        l, _ = d.shape \n",
    "        label = labels[i]\n",
    "        if l < 1200 : # minimum length requirement\n",
    "            break \n",
    "        # generate sampling points\n",
    "        n_point, _ = divmod(l, 1200)\n",
    "        sampling_points = np.arange(start=0, stop=l, step=1200)[:-1]\n",
    "        sampling_points = sampling_points-200\n",
    "        sampling_points[0] = 0\n",
    "        # window sampling \n",
    "        for s in sampling_points:\n",
    "            sub_sample = resample(d[s:s+1200, :], num=120)\n",
    "            all_data.append(sub_sample)\n",
    "            all_ids.append(i+1)\n",
    "            all_labels.append(label)\n",
    "        \n",
    "    return all_data, all_ids, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, all_ids, all_labels = signal_sampling(dataReader.data, dataReader.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768, (120, 51))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data), len(all_ids), all_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mall_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "all_data[[12,4,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_classes = [['watching TV', 'house cleaning', 'standing', 'ascending stairs'], ['walking', 'rope jumping', 'sitting', 'descending stairs'], ['playing soccer', 'lying', 'vacuum cleaning', 'computer work'], ['cycling', 'running', 'Nordic walking'], ['ironing', 'car driving', 'folding laundry']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Load Attribute Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np\n",
    "\n",
    "def load_attribute(fpath):\n",
    "    with open(fpath, \"r\") as pf:\n",
    "        json_data = json.load(pf)\n",
    "\n",
    "    activity_dict = json_data[\"Activity\"]\n",
    "    attribute_dict = json_data[\"Attribute\"]\n",
    "    attr_met = np.array(list(json_data[\"Mark\"].values()))\n",
    "    return activity_dict, attribute_dict, attr_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = load_attribute('../data/OPP_Dataset/OPP_Attributes.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/OPP_Dataset/attribute_ft.npy', 'wb') as pf:\n",
    "    np.save(pf, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "feat_size = 64\n",
    "n_actions = 18 \n",
    "feat_mat = np.random.randn(n_actions, feat_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random \n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F \n",
    "from torch.optim import Adam\n",
    "\n",
    "from scipy.signal import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build PAMAP2 dataset data reader\n",
    "class PAMAP2Reader(object):\n",
    "    def __init__(self, root_path):\n",
    "        self.root_path = root_path\n",
    "        self.readPamap2()\n",
    "\n",
    "    def readFile(self, file_path):\n",
    "        all_data = {\"data\": {}, \"target\": {}, 'collection': []}\n",
    "        prev_action = -1\n",
    "        starting = True\n",
    "        # action_seq = []\n",
    "        action_ID = 0\n",
    "\n",
    "        for l in open(file_path).readlines():\n",
    "            s = l.strip().split()\n",
    "            if s[1] != \"0\":\n",
    "                if (prev_action != int(s[1])):\n",
    "                    if not(starting):\n",
    "                        df = pd.DataFrame(action_seq)\n",
    "                        intep_df = df.interpolate(method='linear', limit_direction='backward', axis=0)\n",
    "                        intep_data = intep_df.values \n",
    "                        all_data['data'][action_ID] = np.array(intep_data)\n",
    "                        all_data['target'][action_ID] = prev_action\n",
    "                        action_ID+=1\n",
    "                    action_seq = []\n",
    "                else:\n",
    "                    starting = False\n",
    "                data_seq = np.array(s[3:]).astype(np.float16)\n",
    "                # data_seq[np.isnan(data_seq)] = 0\n",
    "                action_seq.append(data_seq)\n",
    "                prev_action = int(s[1])\n",
    "                # print(prev_action)\n",
    "                all_data['collection'].append(data_seq)\n",
    "        else: \n",
    "            if len(action_seq) > 1:\n",
    "                df = pd.DataFrame(action_seq)\n",
    "                intep_df = df.interpolate(method='linear', limit_direction='backward', axis=0)\n",
    "                intep_data = intep_df.values\n",
    "                all_data['data'][action_ID] = np.array(intep_data)\n",
    "                all_data['target'][action_ID] = prev_action\n",
    "        return all_data\n",
    "\n",
    "    def readPamap2Files(self, filelist, cols, labelToId):\n",
    "        data = []\n",
    "        labels = []\n",
    "        collection = []\n",
    "        for i, filename in enumerate(filelist):\n",
    "            print('Reading file %d of %d' % (i+1, len(filelist)))\n",
    "            fpath = os.path.join(self.root_path, filename)\n",
    "            file_data = self.readFile(fpath)\n",
    "            data.extend(list(file_data['data'].values()))\n",
    "            labels.extend(list(file_data['target'].values()))\n",
    "            collection.extend(file_data['collection'])\n",
    "        return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n",
    "\n",
    "    def readPamap2(self):\n",
    "        files = ['subject101.dat', 'subject102.dat','subject103.dat','subject104.dat', 'subject105.dat', 'subject106.dat', 'subject107.dat', 'subject108.dat', 'subject109.dat', 'subject110.dat', 'subject111.dat', 'subject112.dat', 'subject113.dat', 'subject114.dat']\n",
    "            \n",
    "        label_map = [\n",
    "            (0, 'other'),\n",
    "            (1, 'lying'),\n",
    "            (2, 'sitting'),\n",
    "            (3, 'standing'),\n",
    "            (4, 'walking'),\n",
    "            (5, 'running'),\n",
    "            (6, 'cycling'),\n",
    "            (7, 'Nordic walking'),\n",
    "            (9, 'watching TV'),\n",
    "            (10, 'computer work'),\n",
    "            (11, 'car driving'),\n",
    "            (12, 'ascending stairs'),\n",
    "            (13, 'descending stairs'),\n",
    "            (16, 'vacuum cleaning'),\n",
    "            (17, 'ironing'),\n",
    "            (18, 'folding laundry'),\n",
    "            (19, 'house cleaning'),\n",
    "            (20, 'playing soccer'),\n",
    "            (24, 'rope jumping')\n",
    "        ]\n",
    "        labelToId = {x[0]: i for i, x in enumerate(label_map)}\n",
    "        # print \"label2id=\",labelToId\n",
    "        idToLabel = [x[1] for x in label_map]\n",
    "        # print \"id2label=\",idToLabel\n",
    "        cols = [\n",
    "                1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
    "                35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53\n",
    "                ]\n",
    "        # print \"cols\",cols\n",
    "        self.data, self.targets, self.all_data = self.readPamap2Files(files, cols, labelToId)\n",
    "        # print(self.data)\n",
    "        # nan_perc = np.isnan(self.data).astype(int).mean()\n",
    "        # print(\"null value percentage \", nan_perc)\n",
    "        # f = lambda x: labelToId[x]\n",
    "        print(np.unique(self.targets))\n",
    "        self.targets = np.array([labelToId[i] for i in list(self.targets)])\n",
    "        print(np.unique(self.targets))\n",
    "        self.label_map = label_map\n",
    "        self.idToLabel = idToLabel\n",
    "        # return data, idToLabel\n",
    "\n",
    "    def resample(self, signal, freq=10):\n",
    "        step_size = int(100/10)\n",
    "        seq_len, _ = signal.shape \n",
    "        resample_indx = np.arange(0, seq_len, step_size)\n",
    "        resampled_sig = signal[resample_indx, :]\n",
    "        return resampled_sig\n",
    "\n",
    "    def windowing(self, signal, window_len, overlap):\n",
    "        seq_len = int(window_len*100) # 100Hz compensation \n",
    "        overlap_len = int(overlap*100) # 100Hz\n",
    "        l, _ = signal.shape\n",
    "        if l > seq_len:\n",
    "            windowing_points = np.arange(start=0, stop=l-seq_len, step=seq_len-overlap_len, dtype=int)[:-1]\n",
    "            # windowing_points = windowing_points-overlap_len\n",
    "            # windowing_points[0] = 0 \n",
    "\n",
    "            windows = [signal[p:p+seq_len, :] for p in windowing_points]\n",
    "        else:\n",
    "            windows = []\n",
    "        return windows\n",
    "\n",
    "    def resampling(self, data, targets, window_size, window_overlap, resample_freq):\n",
    "        assert len(data) == len(targets), \"# action data & # action labels are not matching\"\n",
    "        all_data, all_ids, all_labels = [], [], []\n",
    "        for i, d in enumerate(data):\n",
    "            # print(\">>>>>>>>>>>>>>>  \", np.isnan(d).mean())\n",
    "            label = targets[i]\n",
    "            windows = self.windowing(d, window_size, window_overlap)\n",
    "            for w in windows:\n",
    "                # print(np.isnan(w).mean(), label, i)\n",
    "                resample_sig = self.resample(w, resample_freq)\n",
    "                # print(np.isnan(resample_sig).mean(), label, i)\n",
    "                all_data.append(resample_sig)\n",
    "                all_ids.append(i+1)\n",
    "                all_labels.append(label)\n",
    "\n",
    "        return all_data, all_ids, all_labels\n",
    "\n",
    "    def generate(self, unseen_classes, resampling=True, window_size=5.21, window_overlap=1, resample_freq=10, seen_ratio=0.2, unseen_ratio=0.8):\n",
    "        # assert all([i in list(self.label_map.keys()) for i in unseen_classes]), \"Unknown Class label!\"\n",
    "        seen_classes = [i for i in range(len(self.idToLabel)) if i not in unseen_classes]\n",
    "        unseen_mask = np.in1d(self.targets, unseen_classes)\n",
    "\n",
    "        s = np.unique(self.targets, return_counts=True)\n",
    "        print(\"per class count : \", dict(zip([self.idToLabel[i] for i in s[0]], s[1])))\n",
    "\n",
    "        # build seen dataset \n",
    "        seen_data = self.data[np.invert(unseen_mask)]\n",
    "        seen_targets = self.targets[np.invert(unseen_mask)]\n",
    "        \n",
    "\n",
    "        # build unseen dataset\n",
    "        unseen_data = self.data[unseen_mask]\n",
    "        unseen_targets = self.targets[unseen_mask]\n",
    "\n",
    "        # resampling seen and unseen datasets \n",
    "        seen_data, seen_ids, seen_targets = self.resampling(seen_data, seen_targets, window_size, window_overlap, resample_freq)\n",
    "        unseen_data, unseen_ids, unseen_targets = self.resampling(unseen_data, unseen_targets, window_size, window_overlap, resample_freq)\n",
    "\n",
    "        seen_data, seen_targets = np.array(seen_data), np.array(seen_targets)\n",
    "        unseen_data, unseen_targets = np.array(unseen_data), np.array(unseen_targets)\n",
    "        # train-val split\n",
    "        seen_index = list(range(len(seen_targets)))\n",
    "        random.shuffle(seen_index)\n",
    "        split_point = int((1-seen_ratio)*len(seen_index))\n",
    "        fst_index, sec_index = seen_index[:split_point], seen_index[split_point:]\n",
    "        print(type(fst_index), type(sec_index), type(seen_data), type(seen_targets))\n",
    "        X_seen_train, X_seen_val, y_seen_train, y_seen_val = seen_data[fst_index,:], seen_data[sec_index,:], seen_targets[fst_index], seen_targets[sec_index]\n",
    "        \n",
    "        # val-test split\n",
    "        unseen_index = list(range(len(unseen_targets)))\n",
    "        random.shuffle(unseen_index)\n",
    "        split_point = int((1-unseen_ratio)*len(unseen_index))\n",
    "        fst_index, sec_index = unseen_index[:split_point], unseen_index[split_point:]\n",
    "\n",
    "        X_unseen_val, X_unseen_test, y_unseen_val, y_unseen_test = unseen_data[fst_index,:], unseen_data[sec_index,:], unseen_targets[fst_index], unseen_targets[sec_index]\n",
    "\n",
    "        data = {'train': {\n",
    "                        'X': X_seen_train,\n",
    "                        'y': y_seen_train\n",
    "                        },\n",
    "                'eval-seen':{\n",
    "                        'X': X_seen_val,\n",
    "                        'y': y_seen_val\n",
    "                        },\n",
    "                'eval-unseen':{\n",
    "                        'X': X_unseen_val,\n",
    "                        'y': y_unseen_val\n",
    "                        },\n",
    "                'test': {\n",
    "                        'X': X_unseen_test,\n",
    "                        'y': y_unseen_test\n",
    "                        },\n",
    "                'seen_classes': seen_classes,\n",
    "                'unseen_classes': unseen_classes\n",
    "                }\n",
    "\n",
    "        return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 1 of 14\n",
      "Reading file 2 of 14\n",
      "Reading file 3 of 14\n",
      "Reading file 4 of 14\n",
      "Reading file 5 of 14\n",
      "Reading file 6 of 14\n",
      "Reading file 7 of 14\n",
      "Reading file 8 of 14\n",
      "Reading file 9 of 14\n",
      "Reading file 10 of 14\n",
      "Reading file 11 of 14\n",
      "Reading file 12 of 14\n",
      "Reading file 13 of 14\n",
      "Reading file 14 of 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deela\\AppData\\Local\\Temp\\ipykernel_8772\\2920490930.py:54: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  9 10 11 12 13 16 17 18 19 20 24]\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]\n"
     ]
    }
   ],
   "source": [
    "# import PAMAP2 dataset\n",
    "dataReader = PAMAP2Reader('../data/PAMAP2_Dataset/Protocol/')\n",
    "actionList = dataReader.idToLabel\n",
    "\n",
    "# run 5-fold running\n",
    "fold_classes = [['watching TV', 'house cleaning', 'standing', 'ascending stairs'], ['walking', 'rope jumping', 'sitting', 'descending stairs'], ['playing soccer', 'lying', 'vacuum cleaning', 'computer work'], ['cycling', 'running', 'Nordic walking'], ['ironing', 'car driving', 'folding laundry']]\n",
    "fold_cls_ids = [[actionList.index(i) for i in j] for j in fold_classes]\n",
    "\n",
    "# # separate seen/unseen and train/eval \n",
    "# data_dict = dataReader.generate(unseen_classes=[], resampling=True, seen_ratio=0.2, unseen_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per class count :  {'lying': 8, 'sitting': 8, 'standing': 8, 'walking': 8, 'running': 6, 'cycling': 7, 'Nordic walking': 7, 'watching TV': 1, 'computer work': 4, 'car driving': 1, 'ascending stairs': 16, 'descending stairs': 17, 'vacuum cleaning': 8, 'ironing': 8, 'folding laundry': 4, 'house cleaning': 5, 'playing soccer': 2, 'rope jumping': 6}\n",
      "<class 'list'> <class 'list'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data_dict = dataReader.generate(unseen_classes=[1, 3], resampling=True, seen_ratio=0.2, unseen_ratio=0.8, window_size=5.21, window_overlap=4.21, resample_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['other',\n",
       " 'lying',\n",
       " 'sitting',\n",
       " 'standing',\n",
       " 'walking',\n",
       " 'running',\n",
       " 'cycling',\n",
       " 'Nordic walking',\n",
       " 'watching TV',\n",
       " 'computer work',\n",
       " 'car driving',\n",
       " 'ascending stairs',\n",
       " 'descending stairs',\n",
       " 'vacuum cleaning',\n",
       " 'ironing',\n",
       " 'folding laundry',\n",
       " 'house cleaning',\n",
       " 'playing soccer',\n",
       " 'rope jumping']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataReader.idToLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples :  18252\n",
      "per class count :  {2: 1439, 4: 1832, 5: 771, 6: 1278, 7: 1497, 8: 672, 9: 2431, 10: 437, 11: 874, 12: 763, 13: 1352, 14: 1906, 15: 771, 16: 1486, 17: 372, 18: 371}\n"
     ]
    }
   ],
   "source": [
    "# training dataset\n",
    "train_X, train_y = data_dict['train']['X'], data_dict['train']['y']\n",
    "print(\"number of training samples : \", len(train_y))\n",
    "s = np.unique(train_y, return_counts=True)\n",
    "print(\"per class count : \", dict(zip(s[0], s[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples :  4563\n",
      "per class count :  {2: 368, 4: 511, 5: 178, 6: 327, 7: 345, 8: 159, 9: 646, 10: 102, 11: 207, 12: 190, 13: 357, 14: 435, 15: 205, 16: 356, 17: 86, 18: 91}\n"
     ]
    }
   ],
   "source": [
    "# Seen Evaluation dataset\n",
    "Seval_X, Seval_y = data_dict['eval-seen']['X'], data_dict['eval-seen']['y']\n",
    "print(\"number of training samples : \", len(Seval_y))\n",
    "s = np.unique(Seval_y, return_counts=True)\n",
    "print(\"per class count : \", dict(zip(s[0], s[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples :  746\n",
      "per class count :  {1: 388, 3: 358}\n"
     ]
    }
   ],
   "source": [
    "# Unseen Eval dataset\n",
    "Ueval_X, Ueval_y = data_dict['eval-unseen']['X'], data_dict['eval-unseen']['y']\n",
    "print(\"number of training samples : \", len(Ueval_y))\n",
    "s = np.unique(Ueval_y, return_counts=True)\n",
    "print(\"per class count : \", dict(zip(s[0], s[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples :  2988\n",
      "per class count :  {1: 1492, 3: 1496}\n"
     ]
    }
   ],
   "source": [
    "# Unseen Eval dataset\n",
    "test_X, test_y = data_dict['test']['X'], data_dict['test']['y']\n",
    "print(\"number of training samples : \", len(test_y))\n",
    "s = np.unique(test_y, return_counts=True)\n",
    "print(\"per class count : \", dict(zip(s[0], s[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2988, 53, 51)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['test']['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points :  26549\n",
      "Total number of unseen data :  3734\n",
      "Total number of seen data :  22815\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of data points : \", len(test_y)+len(Ueval_y)+len(Seval_y)+len(train_y))\n",
    "print(\"Total number of unseen data : \", len(test_y)+len(Ueval_y))\n",
    "print(\"Total number of seen data : \", len(Seval_y)+len(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18]),\n",
       " array([1710,  269,  272,  367,  143,  245,  280,  134,  484,   87,  164,\n",
       "         153,  276,  361,  158,  286,   69,   70], dtype=int64))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import action attributes\n",
    "def load_attribute(fpath):\n",
    "    with open(fpath, \"r\") as pf:\n",
    "        json_data = json.load(pf)\n",
    "\n",
    "    activity_dict = json_data[\"Activity\"]\n",
    "    attribute_dict = json_data[\"Attribute\"]\n",
    "    attr_met = np.array(list(json_data[\"Mark\"].values()))\n",
    "    return activity_dict, attribute_dict, attr_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import action semantic features \n",
    "# generate random feature matrix \n",
    "feat_size = 64\n",
    "n_actions = 18 \n",
    "feat_mat = np.random.randn(n_actions, feat_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build mock dataloader\n",
    "class PAMAP2Dataset(Dataset):\n",
    "    def __init__(self, data, actions, attributes, action_feats, action_classes):\n",
    "        super(PAMAP2Dataset, self).__init__()\n",
    "        self.data = torch.from_numpy(data)\n",
    "        self.actions = actions\n",
    "        self.attributes = torch.from_numpy(attributes)\n",
    "        self.action_feats = torch.from_numpy(action_feats)\n",
    "        self.target_feat = torch.from_numpy(action_feats[action_classes, :])\n",
    "        # build action to id mapping dict\n",
    "        self.n_action = len(self.actions)\n",
    "        self.action2Id = dict(zip(action_classes, range(self.n_action)))\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        x = self.data[ind, ...]\n",
    "        x_mask = np.array([0]) #self.padding_mask[ind, ...]\n",
    "        target = self.actions[ind]\n",
    "        y = torch.from_numpy(np.array([self.action2Id[target]]))\n",
    "        y_feat = self.action_feats[target, ...]\n",
    "        attr = self.attributes[target, ...]\n",
    "        return x, y, y_feat, attr, x_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PAMAP2 attributes\n",
    "activity_dict, attribute_dict, attr_mat = load_attribute('./PAMAP2_attributes.json')\n",
    "_, attr_size = attr_mat.shape\n",
    "# import action features\n",
    "feat_size = 64\n",
    "n_actions = 18 \n",
    "feat_mat = np.random.randn(n_actions, feat_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 1 of 9\n",
      "Reading file 2 of 9\n",
      "Reading file 3 of 9\n",
      "Reading file 4 of 9\n",
      "Reading file 5 of 9\n",
      "Reading file 6 of 9\n",
      "Reading file 7 of 9\n",
      "Reading file 8 of 9\n",
      "Reading file 9 of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deela\\AppData\\Local\\Temp\\ipykernel_27504\\1644398372.py:44: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>   3.6782285651230366e-05\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [67], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m fold_cls_ids \u001b[39m=\u001b[39m [[actionList\u001b[39m.\u001b[39mindex(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m j] \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m fold_classes]\n\u001b[0;32m      9\u001b[0m \u001b[39m# separate seen/unseen and train/eval \u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m data_dict \u001b[39m=\u001b[39m dataReader\u001b[39m.\u001b[39;49mgenerate(unseen_classes\u001b[39m=\u001b[39;49m[\u001b[39m1\u001b[39;49m, \u001b[39m4\u001b[39;49m, \u001b[39m14\u001b[39;49m, \u001b[39m2\u001b[39;49m], resampling\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, seen_ratio\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, unseen_ratio\u001b[39m=\u001b[39;49m\u001b[39m0.8\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn [66], line 138\u001b[0m, in \u001b[0;36mPAMAP2Reader.generate\u001b[1;34m(self, unseen_classes, resampling, window_size, window_overlap, resample_freq, seen_ratio, unseen_ratio)\u001b[0m\n\u001b[0;32m    135\u001b[0m unseen_targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets[unseen_mask]\n\u001b[0;32m    137\u001b[0m \u001b[39m# resampling seen and unseen datasets \u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m seen_data, seen_ids, seen_targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresampling(seen_data, seen_targets, window_size, window_overlap, resample_freq)\n\u001b[0;32m    139\u001b[0m unseen_data, unseen_ids, unseen_targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresampling(unseen_data, unseen_targets, window_size, window_overlap, resample_freq)\n\u001b[0;32m    141\u001b[0m seen_data, seen_targets \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(seen_data), np\u001b[39m.\u001b[39marray(seen_targets)\n",
      "Cell \u001b[1;32mIn [66], line 113\u001b[0m, in \u001b[0;36mPAMAP2Reader.resampling\u001b[1;34m(self, data, targets, window_size, window_overlap, resample_freq)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m>>>>>>>>>>>>>>>  \u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39misnan(d)\u001b[39m.\u001b[39mmean())\n\u001b[0;32m    112\u001b[0m label \u001b[39m=\u001b[39m targets[i]\n\u001b[1;32m--> 113\u001b[0m windows \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindowing(d, window_size, window_overlap)\n\u001b[0;32m    114\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m windows:\n\u001b[0;32m    115\u001b[0m     \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39misnan(w)\u001b[39m.\u001b[39mmean(), label, i)\n",
      "Cell \u001b[1;32mIn [66], line 99\u001b[0m, in \u001b[0;36mPAMAP2Reader.windowing\u001b[1;34m(self, signal, window_len, overlap)\u001b[0m\n\u001b[0;32m     97\u001b[0m seq_len \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(window_len\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m) \u001b[39m# 100Hz compensation \u001b[39;00m\n\u001b[0;32m     98\u001b[0m overlap_len \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(overlap\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m) \u001b[39m# 100Hz\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m l, _ \u001b[39m=\u001b[39m signal\u001b[39m.\u001b[39mshape\n\u001b[0;32m    100\u001b[0m windowing_points \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(start\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, stop\u001b[39m=\u001b[39ml, step\u001b[39m=\u001b[39mseq_len, dtype\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m)[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    101\u001b[0m windowing_points \u001b[39m=\u001b[39m windowing_points\u001b[39m-\u001b[39moverlap_len\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# import PAMAP2 dataset\n",
    "dataReader = PAMAP2Reader('../data/PAMAP2_Dataset/Protocol/')\n",
    "actionList = dataReader.idToLabel\n",
    "\n",
    "# run 5-fold running\n",
    "fold_classes = [['watching TV', 'house cleaning', 'standing', 'ascending stairs'], ['walking', 'rope jumping', 'sitting', 'descending stairs'], ['playing soccer', 'lying', 'vacuum cleaning', 'computer work'], ['cycling', 'running', 'Nordic walking'], ['ironing', 'car driving', 'folding laundry']]\n",
    "fold_cls_ids = [[actionList.index(i) for i in j] for j in fold_classes]\n",
    "\n",
    "# separate seen/unseen and train/eval \n",
    "data_dict = dataReader.generate(unseen_classes=[1, 4, 14, 2], resampling=True, seen_ratio=0.2, unseen_ratio=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes = dataReader.idToLabel\n",
    "seen_classes = data_dict['seen_classes']\n",
    "unseen_classes = data_dict['unseen_classes']\n",
    "train_n, seq_len, in_ft = data_dict['train']['X'].shape\n",
    "# build train_dt \n",
    "train_dt = PAMAP2Dataset(data=data_dict['train']['X'], actions=data_dict['train']['y'], attributes=attr_mat, action_feats=feat_mat, action_classes=seen_classes)\n",
    "train_dl = DataLoader(train_dt, batch_size=32, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.  ,  0.43, 32.43, -1.  , 32.  ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, .43, 32.43, np.nan, 32])\n",
    "b = np.nan_to_num(a, nan=-1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44332961724266073"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = data_dict['train']['X']\n",
    "np.isnan(train_data).astype(int).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y,y_feat,attr,mask in train_dt:\n",
    "    print(torch.isnan(x))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_np = np.random.rand(50, 36)\n",
    "sample_df = pd.DataFrame(data=sample_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sample_np.mean(axis=0)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = sample_np.std(axis=0)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.vstack((a,b)).reshape((-1,),order='F')\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random \n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F \n",
    "from torch.optim import Adam\n",
    "\n",
    "from scipy.signal import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build PAMAP2 dataset data reader\n",
    "class PAMAP2Reader(object):\n",
    "    def __init__(self, root_path):\n",
    "        self.root_path = root_path\n",
    "        self.readPamap2()\n",
    "\n",
    "    def readFile(self, file_path):\n",
    "        all_data = {\"data\": {}, \"target\": {}, 'collection': []}\n",
    "        prev_action = -1\n",
    "        starting = True\n",
    "        # action_seq = []\n",
    "        action_ID = 0\n",
    "\n",
    "        for l in open(file_path).readlines():\n",
    "            s = l.strip().split()\n",
    "            if s[1] != \"0\":\n",
    "                if (prev_action != int(s[1])):\n",
    "                    if not(starting):\n",
    "                        df = pd.DataFrame(action_seq)\n",
    "                        intep_df = df.interpolate(method='linear', limit_direction='backward', axis=0)\n",
    "                        intep_data = intep_df.values \n",
    "                        all_data['data'][action_ID] = np.array(intep_data)\n",
    "                        all_data['target'][action_ID] = prev_action\n",
    "                        action_ID+=1\n",
    "                    action_seq = []\n",
    "                else:\n",
    "                    starting = False\n",
    "                data_seq = np.array(s[3:]).astype(np.float16)\n",
    "                # data_seq[np.isnan(data_seq)] = 0\n",
    "                action_seq.append(data_seq)\n",
    "                prev_action = int(s[1])\n",
    "                # print(prev_action)\n",
    "                all_data['collection'].append(data_seq)\n",
    "        else: \n",
    "            if len(action_seq) > 1:\n",
    "                df = pd.DataFrame(action_seq)\n",
    "                intep_df = df.interpolate(method='linear', limit_direction='backward', axis=0)\n",
    "                intep_data = intep_df.values\n",
    "                all_data['data'][action_ID] = np.array(intep_data)\n",
    "                all_data['target'][action_ID] = prev_action\n",
    "        return all_data\n",
    "\n",
    "    def readPamap2Files(self, filelist, cols, labelToId):\n",
    "        data = []\n",
    "        labels = []\n",
    "        collection = []\n",
    "        for i, filename in enumerate(filelist):\n",
    "            print('Reading file %d of %d' % (i+1, len(filelist)))\n",
    "            fpath = os.path.join(self.root_path, filename)\n",
    "            file_data = self.readFile(fpath)\n",
    "            data.extend(list(file_data['data'].values()))\n",
    "            labels.extend(list(file_data['target'].values()))\n",
    "            collection.extend(file_data['collection'])\n",
    "        return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n",
    "\n",
    "    def readPamap2(self):\n",
    "        files = ['subject101.dat', 'subject102.dat','subject103.dat','subject104.dat', 'subject105.dat', 'subject106.dat', 'subject107.dat', 'subject108.dat', 'subject109.dat', 'subject110.dat', 'subject111.dat', 'subject112.dat', 'subject113.dat', 'subject114.dat']\n",
    "            \n",
    "        label_map = [\n",
    "            (0, 'other'),\n",
    "            (1, 'lying'),\n",
    "            (2, 'sitting'),\n",
    "            (3, 'standing'),\n",
    "            (4, 'walking'),\n",
    "            (5, 'running'),\n",
    "            (6, 'cycling'),\n",
    "            (7, 'Nordic walking'),\n",
    "            (9, 'watching TV'),\n",
    "            (10, 'computer work'),\n",
    "            (11, 'car driving'),\n",
    "            (12, 'ascending stairs'),\n",
    "            (13, 'descending stairs'),\n",
    "            (16, 'vacuum cleaning'),\n",
    "            (17, 'ironing'),\n",
    "            (18, 'folding laundry'),\n",
    "            (19, 'house cleaning'),\n",
    "            (20, 'playing soccer'),\n",
    "            (24, 'rope jumping')\n",
    "        ]\n",
    "        labelToId = {x[0]: i for i, x in enumerate(label_map)}\n",
    "        # print \"label2id=\",labelToId\n",
    "        idToLabel = [x[1] for x in label_map]\n",
    "        # print \"id2label=\",idToLabel\n",
    "        cols = [\n",
    "                1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
    "                35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53\n",
    "                ]\n",
    "        # print \"cols\",cols\n",
    "        self.data, self.targets, self.all_data = self.readPamap2Files(files, cols, labelToId)\n",
    "        # print(self.data)\n",
    "        # nan_perc = np.isnan(self.data).astype(int).mean()\n",
    "        # print(\"null value percentage \", nan_perc)\n",
    "        # f = lambda x: labelToId[x]\n",
    "        print(np.unique(self.targets))\n",
    "        self.targets = np.array([labelToId[i] for i in list(self.targets)])\n",
    "        print(np.unique(self.targets))\n",
    "        self.label_map = label_map\n",
    "        self.idToLabel = idToLabel\n",
    "        # return data, idToLabel\n",
    "\n",
    "    def aggregate(self, signal):\n",
    "        means = signal.mean(axis=0)\n",
    "        stds = signal.std(axis=0)\n",
    "        mergered = np.vstack((means,stds)).reshape((-1,),order='F')\n",
    "        return mergered\n",
    "\n",
    "    def windowing(self, signal, window_len, overlap):\n",
    "        seq_len = int(window_len*100) # 100Hz compensation \n",
    "        overlap_len = int(overlap*100) # 100Hz\n",
    "        l, _ = signal.shape\n",
    "        if l > seq_len:\n",
    "            windowing_points = np.arange(start=0, stop=l-seq_len, step=seq_len-overlap_len, dtype=int)[:-1]\n",
    "            # windowing_points = windowing_points-overlap_len\n",
    "            # windowing_points[0] = 0 \n",
    "\n",
    "            windows = [signal[p:p+seq_len, :] for p in windowing_points]\n",
    "        else:\n",
    "            windows = []\n",
    "        return windows\n",
    "\n",
    "    def resampling(self, data, targets, window_size, window_overlap, resample_freq):\n",
    "        assert len(data) == len(targets), \"# action data & # action labels are not matching\"\n",
    "        all_data, all_ids, all_labels = [], [], []\n",
    "        for i, d in enumerate(data):\n",
    "            # print(\">>>>>>>>>>>>>>>  \", np.isnan(d).mean())\n",
    "            label = targets[i]\n",
    "            windows = self.windowing(d, window_size, window_overlap)\n",
    "            for w in windows:\n",
    "                # print(np.isnan(w).mean(), label, i)\n",
    "                resample_sig = self.aggregate(w)\n",
    "                # print(np.isnan(resample_sig).mean(), label, i)\n",
    "                all_data.append(resample_sig)\n",
    "                all_ids.append(i+1)\n",
    "                all_labels.append(label)\n",
    "\n",
    "        return all_data, all_ids, all_labels\n",
    "\n",
    "    def generate(self, unseen_classes, resampling=True, window_size=5.21, window_overlap=1, resample_freq=10, seen_ratio=0.2, unseen_ratio=0.8):\n",
    "        # assert all([i in list(self.label_map.keys()) for i in unseen_classes]), \"Unknown Class label!\"\n",
    "        seen_classes = [i for i in range(len(self.idToLabel)) if i not in unseen_classes]\n",
    "        unseen_mask = np.in1d(self.targets, unseen_classes)\n",
    "\n",
    "        s = np.unique(self.targets, return_counts=True)\n",
    "        print(\"per class count : \", dict(zip([self.idToLabel[i] for i in s[0]], s[1])))\n",
    "\n",
    "        # build seen dataset \n",
    "        seen_data = self.data[np.invert(unseen_mask)]\n",
    "        seen_targets = self.targets[np.invert(unseen_mask)]\n",
    "        \n",
    "\n",
    "        # build unseen dataset\n",
    "        unseen_data = self.data[unseen_mask]\n",
    "        unseen_targets = self.targets[unseen_mask]\n",
    "\n",
    "        # resampling seen and unseen datasets \n",
    "        seen_data, seen_ids, seen_targets = self.resampling(seen_data, seen_targets, window_size, window_overlap, resample_freq)\n",
    "        unseen_data, unseen_ids, unseen_targets = self.resampling(unseen_data, unseen_targets, window_size, window_overlap, resample_freq)\n",
    "\n",
    "        seen_data, seen_targets = np.array(seen_data), np.array(seen_targets)\n",
    "        unseen_data, unseen_targets = np.array(unseen_data), np.array(unseen_targets)\n",
    "        # train-val split\n",
    "        seen_index = list(range(len(seen_targets)))\n",
    "        random.shuffle(seen_index)\n",
    "        split_point = int((1-seen_ratio)*len(seen_index))\n",
    "        fst_index, sec_index = seen_index[:split_point], seen_index[split_point:]\n",
    "        print(type(fst_index), type(sec_index), type(seen_data), type(seen_targets))\n",
    "        X_seen_train, X_seen_val, y_seen_train, y_seen_val = seen_data[fst_index,:], seen_data[sec_index,:], seen_targets[fst_index], seen_targets[sec_index]\n",
    "        \n",
    "        # val-test split\n",
    "        unseen_index = list(range(len(unseen_targets)))\n",
    "        random.shuffle(unseen_index)\n",
    "        split_point = int((1-unseen_ratio)*len(unseen_index))\n",
    "        fst_index, sec_index = unseen_index[:split_point], unseen_index[split_point:]\n",
    "\n",
    "        X_unseen_val, X_unseen_test, y_unseen_val, y_unseen_test = unseen_data[fst_index,:], unseen_data[sec_index,:], unseen_targets[fst_index], unseen_targets[sec_index]\n",
    "\n",
    "        data = {'train': {\n",
    "                        'X': X_seen_train,\n",
    "                        'y': y_seen_train\n",
    "                        },\n",
    "                'eval-seen':{\n",
    "                        'X': X_seen_val,\n",
    "                        'y': y_seen_val\n",
    "                        },\n",
    "                'eval-unseen':{\n",
    "                        'X': X_unseen_val,\n",
    "                        'y': y_unseen_val\n",
    "                        },\n",
    "                'test': {\n",
    "                        'X': X_unseen_test,\n",
    "                        'y': y_unseen_test\n",
    "                        },\n",
    "                'seen_classes': seen_classes,\n",
    "                'unseen_classes': unseen_classes\n",
    "                }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 1 of 14\n",
      "Reading file 2 of 14\n",
      "Reading file 3 of 14\n",
      "Reading file 4 of 14\n",
      "Reading file 5 of 14\n",
      "Reading file 6 of 14\n",
      "Reading file 7 of 14\n",
      "Reading file 8 of 14\n",
      "Reading file 9 of 14\n",
      "Reading file 10 of 14\n",
      "Reading file 11 of 14\n",
      "Reading file 12 of 14\n",
      "Reading file 13 of 14\n",
      "Reading file 14 of 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deela\\AppData\\Local\\Temp\\ipykernel_35016\\2902715235.py:54: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  9 10 11 12 13 16 17 18 19 20 24]\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]\n"
     ]
    }
   ],
   "source": [
    "dataReader = PAMAP2Reader('../data/PAMAP2_Dataset/Protocol/')\n",
    "actionList = dataReader.idToLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per class count :  {'lying': 8, 'sitting': 8, 'standing': 8, 'walking': 8, 'running': 6, 'cycling': 7, 'Nordic walking': 7, 'watching TV': 1, 'computer work': 4, 'car driving': 1, 'ascending stairs': 16, 'descending stairs': 17, 'vacuum cleaning': 8, 'ironing': 8, 'folding laundry': 4, 'house cleaning': 5, 'playing soccer': 2, 'rope jumping': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\numpy\\core\\_methods.py:247: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n",
      "c:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\numpy\\core\\_methods.py:213: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data_dict = dataReader.generate(unseen_classes=[1, 3], resampling=True, seen_ratio=0.2, unseen_ratio=0.8, window_size=5.21, window_overlap=4.21, resample_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18252, 102)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['train']['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mvts_trans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
