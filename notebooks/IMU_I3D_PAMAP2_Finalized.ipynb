{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ip9_bjhc3cyroaBfFa8ADeZGu5N4vUxB","timestamp":1678625861476},{"file_id":"124a2JPGUhU77cz2AUqeMkTrU1J033pNj","timestamp":1678624538058},{"file_id":"1GdillffPEPp1rguAJVpEDCu_FPLx8_fM","timestamp":1678505352614},{"file_id":"1KPEw9iWWVmGhlhrQKGlR_Bs7byx1TCH_","timestamp":1678334206425},{"file_id":"1CTJmtd3zarhF9M3fOk5g9zS6mZKy2XP8","timestamp":1677944836036}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"8601a6a245fd4f7eadc97b48d10cd54b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6511fb4c054f45a4b509a43339666c3f","IPY_MODEL_0e1b2786f0e44c2d9d4aab1f453230e1","IPY_MODEL_d6bbe1656fb74a0db1178dd05e4741fd"],"layout":"IPY_MODEL_2599a0dad80047da89ef69a86b031126"}},"6511fb4c054f45a4b509a43339666c3f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd09bb87351b46c382978a491f226e6d","placeholder":"​","style":"IPY_MODEL_53e4e9b5912e4feb826f86eba5849cbd","value":"Training Epoch:   0%"}},"0e1b2786f0e44c2d9d4aab1f453230e1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_7df631a1c8364c51a93422a2c4116e0f","max":15,"min":0,"orientation":"horizontal","style":"IPY_MODEL_36391b3d917149e1bb2357dd382cedbb","value":0}},"d6bbe1656fb74a0db1178dd05e4741fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09e5693563094338a1a06f22cea202a0","placeholder":"​","style":"IPY_MODEL_9128a4798a0249f7b9acfb404b277190","value":" 0/15 [00:04&lt;?, ?it/s]"}},"2599a0dad80047da89ef69a86b031126":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd09bb87351b46c382978a491f226e6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53e4e9b5912e4feb826f86eba5849cbd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7df631a1c8364c51a93422a2c4116e0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36391b3d917149e1bb2357dd382cedbb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"09e5693563094338a1a06f22cea202a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9128a4798a0249f7b9acfb404b277190":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3414947a3a0e403092050bdfb7e4d7ca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_04bef2bd09cc436bb64d17c716b23a21","IPY_MODEL_095934a18a3a4dbe8595ccfa8f54fedb","IPY_MODEL_1a1dd862bfc146408557a065671af63b"],"layout":"IPY_MODEL_07e2e431cb6840f49812e20bf4832532"}},"04bef2bd09cc436bb64d17c716b23a21":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_acc858d7b15040ff925818726250ccd5","placeholder":"​","style":"IPY_MODEL_827714c3c4fb4eafac1377ac4adc975b","value":"train:  17%"}},"095934a18a3a4dbe8595ccfa8f54fedb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_885e8b0d99174ef3bcff55b46b7d24a6","max":588,"min":0,"orientation":"horizontal","style":"IPY_MODEL_402043caafa343e389a127a748024a5a","value":102}},"1a1dd862bfc146408557a065671af63b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_486d50d223794b0a93f942fe668d0a3d","placeholder":"​","style":"IPY_MODEL_5e4564f92bb54cb397891e7d71d2b6d3","value":" 102/588 [00:04&lt;00:14, 32.96batch/s, loss=46.1, accuracy=0.219]"}},"07e2e431cb6840f49812e20bf4832532":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"acc858d7b15040ff925818726250ccd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"827714c3c4fb4eafac1377ac4adc975b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"885e8b0d99174ef3bcff55b46b7d24a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"402043caafa343e389a127a748024a5a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"486d50d223794b0a93f942fe668d0a3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e4564f92bb54cb397891e7d71d2b6d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"NJhThY8PC2si","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678813669553,"user_tz":-330,"elapsed":31872,"user":{"displayName":"Pathirage Nipun Deelaka","userId":"00264959805100472870"}},"outputId":"d255d73f-8014-49c8-872d-5dcd96073c21"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["! pip install neptune\n","! git clone https://github.com/nipdep/HAR-ZSL-XAI.git --branch pd/PoseAE --single-branch\n","! mv /content/HAR-ZSL-XAI/src /content/"],"metadata":{"id":"khVPupusDG8T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678813695054,"user_tz":-330,"elapsed":25508,"user":{"displayName":"Pathirage Nipun Deelaka","userId":"00264959805100472870"}},"outputId":"f9a41210-adce-43f2-bad5-83d6ca915c17"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting neptune\n","  Downloading neptune-1.0.2-py3-none-any.whl (443 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/443.8 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 KB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting PyJWT\n","  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from neptune) (1.26.15)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from neptune) (5.4.8)\n","Collecting bravado<12.0.0,>=11.0.0\n","  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n","Collecting swagger-spec-validator>=2.7.4\n","  Downloading swagger_spec_validator-3.0.3-py2.py3-none-any.whl (27 kB)\n","Collecting websocket-client!=1.0.0,>=0.35.0\n","  Downloading websocket_client-1.5.1-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from neptune) (23.0)\n","Collecting future>=0.17.1\n","  Downloading future-0.18.3.tar.gz (840 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 KB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from neptune) (1.3.1)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from neptune) (1.15.0)\n","Collecting GitPython>=2.0.8\n","  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from neptune) (3.2.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from neptune) (1.4.4)\n","Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.9/dist-packages (from neptune) (8.4.0)\n","Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.9/dist-packages (from neptune) (2.25.1)\n","Collecting boto3>=1.16.0\n","  Downloading boto3-1.26.90-py3-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.7/134.7 KB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from neptune) (8.1.3)\n","Collecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting botocore<1.30.0,>=1.29.90\n","  Downloading botocore-1.29.90-py3-none-any.whl (10.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Requirement already satisfied: msgpack in /usr/local/lib/python3.9/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.0.5)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (2.8.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (4.5.0)\n","Collecting bravado-core>=5.16.1\n","  Downloading bravado_core-5.17.1-py2.py3-none-any.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting simplejson\n","  Downloading simplejson-3.18.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.8/136.8 KB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting monotonic\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20.0->neptune) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20.0->neptune) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20.0->neptune) (2.10)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.9/dist-packages (from swagger-spec-validator>=2.7.4->neptune) (4.3.3)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->neptune) (2022.7.1)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas->neptune) (1.22.4)\n","Collecting jsonref\n","  Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n","Collecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.19.3)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (22.2.0)\n","Collecting rfc3339-validator\n","  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n","Collecting rfc3987\n","  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n","Collecting webcolors>=1.11\n","  Downloading webcolors-1.12-py3-none-any.whl (9.9 kB)\n","Collecting isoduration\n","  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n","Collecting uri-template\n","  Downloading uri_template-1.2.0-py3-none-any.whl (10 kB)\n","Collecting jsonpointer>1.13\n","  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n","Collecting fqdn\n","  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n","Collecting arrow>=0.15.0\n","  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: future\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492037 sha256=62248f77a6e0e3e12eed6a322327de9b56ac824a8b38b09845db1d3b7d214365\n","  Stored in directory: /root/.cache/pip/wheels/bf/5d/6a/2e53874f7ec4e2bede522385439531fafec8fafe005b5c3d1b\n","Successfully built future\n","Installing collected packages: rfc3987, monotonic, websocket-client, webcolors, uri-template, smmap, simplejson, rfc3339-validator, PyJWT, jsonref, jsonpointer, jmespath, future, fqdn, swagger-spec-validator, gitdb, botocore, arrow, s3transfer, isoduration, GitPython, boto3, bravado-core, bravado, neptune\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","Successfully installed GitPython-3.1.31 PyJWT-2.6.0 arrow-1.2.3 boto3-1.26.90 botocore-1.29.90 bravado-11.0.3 bravado-core-5.17.1 fqdn-1.5.1 future-0.18.3 gitdb-4.0.10 isoduration-20.11.0 jmespath-1.0.1 jsonpointer-2.3 jsonref-1.1.0 monotonic-1.6 neptune-1.0.2 rfc3339-validator-0.1.4 rfc3987-1.3.8 s3transfer-0.6.0 simplejson-3.18.3 smmap-5.0.0 swagger-spec-validator-3.0.3 uri-template-1.2.0 webcolors-1.12 websocket-client-1.5.1\n","Cloning into 'HAR-ZSL-XAI'...\n","remote: Enumerating objects: 206, done.\u001b[K\n","remote: Counting objects: 100% (206/206), done.\u001b[K\n","remote: Compressing objects: 100% (117/117), done.\u001b[K\n","remote: Total 206 (delta 97), reused 189 (delta 87), pack-reused 0\u001b[K\n","Receiving objects: 100% (206/206), 64.85 MiB | 33.35 MiB/s, done.\n","Resolving deltas: 100% (97/97), done.\n"]}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"5G3REk2jDHN1"}},{"cell_type":"code","source":["data_root = '/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Datasets/Consolidated/PAMPA2'"],"metadata":{"id":"PjkkcslBunGA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os \n","from datetime import date, datetime\n","from tqdm.autonotebook import tqdm\n","from copy import deepcopy\n","from collections import defaultdict\n","import numpy as np \n","import numpy.random as random\n","import pandas as pd\n","import json\n","import pickle\n","from collections import defaultdict, OrderedDict\n","import neptune\n","\n","import torch \n","from torch import nn, Tensor\n","from torch.nn import functional as F\n","from torch.nn.modules import MultiheadAttention, Linear, Dropout, BatchNorm1d, TransformerEncoderLayer\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import Adam\n","from torch.nn import MSELoss\n","\n","\n","from src.datasets.data import PAMAP2Reader, PAMAP2ReaderV2\n","# from src.datasets.dataset import PAMAP2Dataset\n","from src.utils.analysis import action_evaluator\n","from src.datasets.utils import load_attribute\n","\n","from src.models.loss import FeatureLoss, AttributeLoss\n","from src.utils.losses import *\n","from src.utils.analysis import action_evaluator\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# from src.running import train_step1, eval_step1"],"metadata":{"id":"DWgMHHXXDG5n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678813700913,"user_tz":-330,"elapsed":5872,"user":{"displayName":"Pathirage Nipun Deelaka","userId":"00264959805100472870"}},"outputId":"b5be9350-b405-4793-ec2b-d910ac4793d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-4-151b9d639ad9>:3: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm\n"]}]},{"cell_type":"code","source":["from sklearn.manifold import TSNE\n","# from umap import UMAP\n","\n","import matplotlib.pyplot as plt \n","import seaborn as sns \n","import plotly.express as px"],"metadata":{"id":"enxG_kfLUdK9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# setup model configurations\n","config = {\n","    # general information\n","    \"datetime\": date.today(),\n","    \"device\": \"gpu\",\n","    \"dataset\": \"PAMAP2\", # \"PAMAP2\", \"DaLiAc\", \"UTD\"\n","    \"Model\": \"BiLSTM\",\n","    \"sem-space\": 'I3D',\n","    # model training configs\n","    \"lr\": 0.001,\n","    \"imu_alpha\": 0.0001,\n","    \"n_epochs\": 15,\n","    \"batch_size\": 32,\n","    # model configs\n","    \"d_model\": 128, \n","    \"num_heads\": 2,\n","    \"feat_size\": 400, # skel-AE hidden size and IMU-Anc output size\n","    \"semantic_size\": 400,\n","    # dataset configs\n","    \"window_size\": 5.21, \n","    \"overlap\": 4.21,\n","    \"freq\": 50,\n","    \"seq_len\": 50,  # skeleton seq. length\n","    \"seen_split\": 0.1,\n","}"],"metadata":{"id":"umHwAr5pDG2N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_model(model,model_name,unique_name,fold_id):\n","    PATH = f\"{models_saves}/{model_name}\"\n","    os.makedirs(PATH,exist_ok=True)\n","    torch.save({\n","        \"n_epochs\" : config[\"n_epochs\"],\n","        \"model_state_dict\":model.state_dict(),\n","        \"config\": config\n","    }, f\"{PATH}/{unique_name}_{fold_id}.pt\")\n","\n","model_iden = \"fold\"\n","notebook_iden = \"SORTModel_feature\"\n","models_saves = \"model_saves\""],"metadata":{"id":"HpuaMQupDNPI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["IMU_data_path = data_root+'/IMU/Protocol/'\n","dataReader = PAMAP2ReaderV2(IMU_data_path)\n","actionList = dataReader.idToLabel"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qQcUv5ikFZyN","executionInfo":{"status":"ok","timestamp":1678813868529,"user_tz":-330,"elapsed":167055,"user":{"displayName":"Pathirage Nipun Deelaka","userId":"00264959805100472870"}},"outputId":"be8d5ad9-8b6c-4d59-efcc-dfa53c107aeb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading file 1 of 14\n","Reading file 2 of 14\n","Reading file 3 of 14\n","Reading file 4 of 14\n","Reading file 5 of 14\n","Reading file 6 of 14\n","Reading file 7 of 14\n","Reading file 8 of 14\n","Reading file 9 of 14\n","Reading file 10 of 14\n","Reading file 11 of 14\n","Reading file 12 of 14\n","Reading file 13 of 14\n","Reading file 14 of 14\n"]},{"output_type":"stream","name":"stderr","text":["/content/src/datasets/data.py:1275: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n"]}]},{"cell_type":"code","source":["def read_I3D_pkl(loc,feat_size=\"400\"):\n","  if feat_size == \"400\":\n","    feat_index = 1\n","  elif feat_size == \"2048\":\n","    feat_index = 0\n","  else:\n","    raise NotImplementedError()\n","\n","  with open(loc,\"rb\") as f0:\n","    __data = pickle.load(f0)\n","\n","  label = []\n","  prototype = []\n","  for k,v in __data.items():\n","    label.append(k)\n","    all_arr = [x[feat_index] for x in v]\n","    all_arr = np.asarray(all_arr).mean(axis=0)\n","    prototype.append(all_arr)\n","\n","  label = np.asarray(label)\n","  prototype = np.array(prototype)\n","  return {\"activity\":label, \"features\":prototype}"],"metadata":{"id":"NZxOPxsMhyEF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load video dataset\n","I3D_data_path  = data_root + '/I3D/video_feat.pkl'\n","video_data = read_I3D_pkl(I3D_data_path, feat_size=\"400\")\n","video_classes, attr_mat = video_data['activity'], video_data['features']"],"metadata":{"id":"YAO5neZHGJKL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# re-arrange semantic space\n","activity_dict = dict(zip(video_classes, attr_mat))\n","semantic_space = np.array([activity_dict[c] for c in actionList])"],"metadata":{"id":"MqZLOjZrQLOf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PAMAP2Dataset(Dataset):\n","    def __init__(self, data, actions, attributes, action_classes, seq_len=120):\n","        super(PAMAP2Dataset, self).__init__()\n","        self.data = torch.from_numpy(data)\n","        self.actions = actions\n","        self.seq_len = seq_len\n","        self.attributes = torch.from_numpy(attributes)\n","        self.action_classes = action_classes\n","        # build action to id mapping dict\n","        self.n_action = len(self.actions)\n","        self.action2Id = dict(zip(action_classes, range(self.n_action)))\n","\n","    def __getitem__(self, ind):\n","        x = self.data[ind, ...]\n","        target = self.actions[ind]\n","        y = torch.from_numpy(np.array([self.action2Id[target]]))\n","        # extraction semantic space generation skeleton sequences\n","        y_feat = self.attributes[target, ...]\n","        return x, y, y_feat\n","\n","    def __len__(self):\n","        return self.data.shape[0]\n","\n","    def getClassAttrs(self):\n","        sampling_idx = [random.choice(self.attribute_dict[i]) for i in self.action_classes]\n","        ft_mat = self.attributes[sampling_idx, ...]\n","        return ft_mat\n","\n","    def getClassFeatures(self):\n","        cls_feat = self.attributes[self.action_classes, ...]\n","        return cls_feat"],"metadata":{"id":"oMgevInhKbjy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class IMUEncoder(nn.Module):\n","    def __init__(self, in_ft, d_model, ft_size, n_classes, num_heads=1, max_len=1024, dropout=0.1):\n","        super(IMUEncoder, self).__init__()\n","        self.in_ft = in_ft\n","        self.max_len = max_len\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.ft_size = ft_size \n","        self.n_classes = n_classes\n","\n","        self.lstm = nn.LSTM(input_size=self.in_ft,\n","                            hidden_size=self.d_model,\n","                            num_layers=self.num_heads,\n","                            batch_first=True,\n","                            bidirectional=True)\n","        self.drop = nn.Dropout(p=0.1)\n","        self.act = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","        self.fcLayer1 = nn.Linear(2*self.d_model, self.ft_size)\n","        # self.fcLayer2 = nn.Linear(self.ft_size, self.ft_size)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out_forward = out[:, self.max_len - 1, :self.d_model]\n","        out_reverse = out[:, 0, self.d_model:]\n","        out_reduced = torch.cat((out_forward, out_reverse), 1)\n","        out = self.drop(out_reduced)\n","        out = self.act(out)\n","        out = self.fcLayer1(out)\n","        out = self.sigmoid(out)\n","        # out = self.fcLayer2(out)\n","        return out"],"metadata":{"id":"FIWrGZwzUnbQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"RKpMzwHKGAQ2"}},{"cell_type":"code","source":["if config['device'] == 'cpu':\n","    device = \"cpu\"\n","else:\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"NsH-K0HpF7tu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run 5-fold running\n","fold_classes = [['watching TV', 'house cleaning', 'standing', 'ascending stairs'], ['walking', 'rope jumping', 'sitting', 'descending stairs'], ['playing soccer', 'lying', 'vacuum cleaning', 'computer work'], ['cycling', 'running', 'Nordic walking'], ['ironing', 'car driving', 'folding laundry']]\n","\n","fold_cls_ids = [[actionList.index(i) for i in j] for j in fold_classes]"],"metadata":{"id":"c9Lp6g_IGCJs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def loss_cross_entropy(y_pred, cls, selected_features, loss_fn=nn.CrossEntropyLoss(reduction=\"mean\")):\n","    # print(\"y_pred > \", y_pred.shape, \"cls > \", cls.shape, \"selected_features > \", selected_features.shape)\n","    num_classes = selected_features.size()[0]\n","\n","    cosine_sim_comb = []\n","    for entry in y_pred.unbind():\n","        #print(entry.repeat(num_classes,1),selected_features.size())\n","        cosine_sim = F.softmax(torch.abs(F.cosine_similarity(entry.repeat(num_classes,1),selected_features)),dim=-1)\n","        cosine_sim_comb.append(cosine_sim)\n","\n","    cosine_sim_comb = torch.stack(cosine_sim_comb)\n","    loss = loss_fn(cosine_sim_comb,cls)\n","    return loss\n","\n","def loss_reconstruction_calc(y_pred, y_feat, loss_fn=nn.L1Loss(reduction=\"sum\")):\n","    bat_size,feature_size = y_pred.size()\n","    loss = loss_fn(y_pred,y_feat)*(1/feature_size)\n","    return loss\n","\n","def predict_class(y_pred, selected_features):\n","\n","    num_classes = selected_features.size()[0]\n","\n","    cosine_sim_comb = []\n","    for entry in y_pred.unbind():\n","        cosine_sim = torch.argmax(F.softmax(torch.abs(F.cosine_similarity(entry.repeat(num_classes,1),selected_features)),dim=-1))\n","        cosine_sim_comb.append(cosine_sim)\n","\n","    pred = torch.stack(cosine_sim_comb)\n","    return pred"],"metadata":{"id":"FBzAK08GHu6z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_step(model, dataloader, dataset:PAMAP2Dataset, optimizer, loss_module, device, class_names, phase='train', l2_reg=False, loss_alpha=0.7):\n","    model = model.train()\n","    epoch_loss = 0  # total loss of epoch\n","    total_samples = 0  # total samples in epoch\n","    random_selected_feat = dataset.getClassFeatures().to(device)\n","\n","    with tqdm(dataloader, unit=\"batch\", desc=phase) as tepoch:\n","        for batch in tepoch:\n","            X, targets, target_feat = batch\n","            X = X.float().to(device)\n","            target_feat = target_feat.float().to(device)\n","            targets = targets.long().to(device)\n","\n","            # Zero gradients, perform a backward pass, and update the weights.\n","            optimizer.zero_grad()\n","            # forward track history if only in train\n","            with torch.set_grad_enabled(phase == 'train'):\n","            # with autocast():\n","                feat_output = model(X)\n","                class_loss = loss_cross_entropy(feat_output, targets.squeeze(), random_selected_feat, loss_fn =loss_module['class'] )\n","                feat_loss = loss_reconstruction_calc(feat_output, target_feat, loss_fn=loss_module[\"feature\"])\n","\n","            #loss = cross_entropy_loss\n","            loss = feat_loss + loss_alpha*class_loss\n","            class_output = predict_class(feat_output, random_selected_feat)\n","\n","            if phase == 'train':\n","                loss.backward()\n","                optimizer.step()\n","\n","            metrics = {\"loss\": loss.item()}\n","            with torch.no_grad():\n","                total_samples += len(targets)\n","                epoch_loss += loss.item()  # add total loss of batch\n","\n","            # convert feature vector into action class using cosine\n","            pred_class = class_output.cpu().detach().numpy()\n","            metrics[\"accuracy\"] = accuracy_score(y_true=targets.cpu().detach().numpy(), y_pred=pred_class)\n","            tepoch.set_postfix(metrics)\n","\n","    epoch_loss = epoch_loss / total_samples  # average loss per sample for whole epoch\n","    return metrics"],"metadata":{"id":"NOS3PISDH6ir"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eval_step(model, dataloader,dataset, loss_module, device, class_names,  phase='seen', l2_reg=False, print_report=True, loss_alpha=0.7):\n","    model = model.eval()\n","    epoch_loss = 0  # total loss of epoch\n","    total_samples = 0  # total samples in epoch\n","    random_selected_feat = dataset.getClassFeatures().to(device)\n","    per_batch = {'target_masks': [], 'targets': [], 'predictions': [], 'metrics': [], 'IDs': []}\n","    metrics = {\"samples\": 0, \"loss\": 0, \"feat. loss\": 0, \"classi. loss\": 0}\n","\n","    with tqdm(dataloader, unit=\"batch\", desc=phase) as tepoch:\n","        for batch in tepoch:\n","            X, targets, target_feat = batch\n","            X = X.float().to(device)\n","            target_feat = target_feat.float().to(device)\n","            targets = targets.long().to(device)\n","\n","            # forward track history if only in train\n","            with torch.set_grad_enabled(phase == 'train'):\n","            # with autocast():\n","                feat_output = model(X)\n","                class_loss = loss_cross_entropy(feat_output, targets.squeeze(), random_selected_feat, loss_fn =loss_module['class'] )\n","                feat_loss = loss_reconstruction_calc(feat_output, target_feat, loss_fn=loss_module[\"feature\"])\n","            \n","            loss = feat_loss + loss_alpha*class_loss\n","            class_output = predict_class(feat_output, random_selected_feat)\n","\n","            # convert feature vector into action class using cosine\n","            if phase == 'seen':\n","                pred_action = class_output\n","            else:\n","                #feat_numpy = torch.sigmoid(feat_output.cpu().detach())\n","                #action_probs = cosine_similarity(feat_numpy, target_feat_met)\n","                pred_action = class_output\n","\n","            with torch.no_grad():\n","                metrics['samples'] += len(targets)\n","                metrics['loss'] += loss.item()  # add total loss of batch\n","                metrics['feat. loss'] += feat_loss.item()\n","                metrics['classi. loss'] += class_loss.item()\n","\n","            per_batch['targets'].append(targets.cpu().numpy())\n","            per_batch['predictions'].append(pred_action.cpu().numpy())\n","            per_batch['metrics'].append([loss.cpu().numpy()])\n","\n","            tepoch.set_postfix({\"loss\": loss.item()})\n","\n","    all_preds = np.concatenate(per_batch[\"predictions\"])\n","    all_targets = np.concatenate(per_batch[\"targets\"])\n","    metrics_dict = action_evaluator(y_pred=all_preds, y_true=all_targets[:, 0], class_names=class_names, print_report=print_report)\n","    metrics_dict.update(metrics)\n","    return metrics_dict"],"metadata":{"id":"jYNZfoaMH_Xr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_curves(df):\n","    df['loss'] = df['loss']/df['samples']\n","    df['feat. loss'] = df['feat. loss']/df['samples']\n","    df['classi. loss'] = df['classi. loss']/df['samples']\n","    \n","    fig, axs = plt.subplots(nrows=4)\n","    sns.lineplot(data=df, x='epoch', y='loss', hue='phase', marker='o', ax=axs[2]).set(title=\"Loss\")\n","    sns.lineplot(data=df, x='epoch', y='feat. loss', hue='phase', marker='o', ax=axs[0]).set(title=\"Feature Loss\")\n","    sns.lineplot(data=df, x='epoch', y='classi. loss', hue='phase', marker='o', ax=axs[1]).set(title=\"Classification Loss\")\n","    sns.lineplot(data=df, x='epoch', y='accuracy', hue='phase', marker='o', ax=axs[3]).set(title=\"Accuracy\")"],"metadata":{"id":"xf2ful7mTOix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def log(fold, phase, metrics):\n","    for m, v in metrics.items():\n","        if fold == 'global':\n","            run[f'global/{m}'].log(v)\n","        else:\n","            run[f\"Fold-{fold}/{phase}/{m}\"].log(v) "],"metadata":{"id":"8IdfvX3s11U7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["run = neptune.init_run(\n","    project=\"FYP-Group22/ICANN-Logs\",\n","    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJkNWJjMDdhNC05NWY5LTQwNWQtYTQyNi0zNjNmYmYwZDg3M2YifQ==\",\n",")  # your credentials"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZOctuOEp1xlx","executionInfo":{"status":"ok","timestamp":1678813871976,"user_tz":-330,"elapsed":1532,"user":{"displayName":"Pathirage Nipun Deelaka","userId":"00264959805100472870"}},"outputId":"48f6af6d-e524-432e-a561-bf52563d2bbc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-21-0fd58e113600>:1: NeptuneWarning: To avoid unintended consumption of logging hours during interactive sessions, the following monitoring options are disabled unless set to 'True' when initializing the run: 'capture_stdout', 'capture_stderr', and 'capture_hardware_metrics'.\n","  run = neptune.init_run(\n"]},{"output_type":"stream","name":"stdout","text":["https://app.neptune.ai/FYP-Group22/ICANN-Logs/e/IC-22\n"]}]},{"cell_type":"code","source":["# run['parameters'] = config\n","fold_metric_scores = []\n","\n","for i, cs in enumerate(fold_cls_ids):\n","    print(\"=\"*16, f'Fold-{i}', \"=\"*16)\n","    print(f'Unseen Classes : {fold_classes[i]}')\n","\n","    data_dict = dataReader.generate(unseen_classes=cs, seen_ratio=config['seen_split'], unseen_ratio=0.8, window_size=config['window_size'], window_overlap=config['overlap'], resample_freq=config['freq'])\n","    all_classes = dataReader.idToLabel\n","    seen_classes = data_dict['seen_classes']\n","    unseen_classes = data_dict['unseen_classes']\n","    print(\"seen classes > \", seen_classes)\n","    print(\"unseen classes > \", unseen_classes)\n","    train_n, seq_len, in_ft = data_dict['train']['X'].shape\n","\n","    print(\"Initiate IMU datasets ...\")\n","    # build IMU datasets\n","    train_dt = PAMAP2Dataset(data=data_dict['train']['X'], actions=data_dict['train']['y'], attributes=semantic_space, action_classes=seen_classes, seq_len=100)\n","    train_dl = DataLoader(train_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n","    # build seen eval_dt\n","    eval_dt = PAMAP2Dataset(data=data_dict['eval-seen']['X'], actions=data_dict['eval-seen']['y'], attributes=semantic_space, action_classes=seen_classes, seq_len=100)\n","    eval_dl = DataLoader(eval_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n","    # build unseen test_dt\n","    test_dt = PAMAP2Dataset(data=data_dict['test']['X'], actions=data_dict['test']['y'], attributes=semantic_space, action_classes=unseen_classes, seq_len=100)\n","    test_dl = DataLoader(test_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n","    \n","    # build model\n","    imu_config = {\n","        'in_ft':in_ft, \n","        'd_model':config['d_model'], \n","        'num_heads':config['num_heads'], \n","        'ft_size':config['feat_size'], \n","        'max_len':seq_len, \n","        'n_classes':len(seen_classes)\n","    }\n","    model = IMUEncoder(**imu_config)\n","    model.to(device)\n","\n","    # define run parameters \n","    optimizer = Adam(model.parameters(), lr=config['lr'], weight_decay=1e-5)\n","    loss_module = {'class': nn.CrossEntropyLoss(reduction=\"sum\"), 'feature': nn.L1Loss(reduction=\"sum\")}\n","    best_acc = 0.0\n","\n","    # train the model \n","    train_data = []\n","    for epoch in tqdm(range(config['n_epochs']), desc='Training Epoch', leave=False):\n","    \n","        train_metrics = train_step(model, train_dl, train_dt,optimizer, loss_module, device, class_names=[all_classes[i] for i in seen_classes], phase='train', loss_alpha=0.0001)\n","        train_metrics['epoch'] = epoch\n","        train_metrics['phase'] = 'train'\n","        train_data.append(train_metrics)\n","        log(i, 'train', train_metrics)\n","\n","        eval_metrics = eval_step(model, eval_dl, eval_dt,loss_module, device, class_names=[all_classes[i] for i in seen_classes], phase='seen', loss_alpha=0.0001, print_report=False)\n","        eval_metrics['epoch'] = epoch \n","        eval_metrics['phase'] = 'valid'\n","        train_data.append(eval_metrics)\n","        log(i, 'valid', train_metrics)\n","        # print(f\"EPOCH [{epoch}] TRAINING : {train_metrics}\")\n","        # print(f\"EPOCH [{epoch}] EVAL : {eval_metrics}\")\n","        if eval_metrics['accuracy'] > best_acc:\n","            best_model = deepcopy(model.state_dict())\n","    \n","    train_df = pd.DataFrame().from_records(train_data)\n","    plot_curves(train_df)\n","\n","    # replace by best model \n","    model.load_state_dict(best_model)\n","    # save_model(model,notebook_iden,model_iden,i)\n","\n","    # run evaluation on unseen classes\n","    test_metrics = eval_step(model, test_dl,test_dt, loss_module, device, class_names=[all_classes[i] for i in unseen_classes], phase='unseen', loss_alpha=0.0001, print_report=False)\n","    fold_metric_scores.append(test_metrics)\n","    log('test', i, test_metrics)\n","    print(test_metrics)\n","    print(\"=\"*40)\n","\n","print(\"=\"*14, \"Overall Unseen Classes Performance\", \"=\"*14)\n","seen_score_df = pd.DataFrame.from_records(fold_metric_scores)\n","print(seen_score_df.mean())\n","log('global', '',seen_score_df.mean().to_dict())\n","run.stop()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["8601a6a245fd4f7eadc97b48d10cd54b","6511fb4c054f45a4b509a43339666c3f","0e1b2786f0e44c2d9d4aab1f453230e1","d6bbe1656fb74a0db1178dd05e4741fd","2599a0dad80047da89ef69a86b031126","dd09bb87351b46c382978a491f226e6d","53e4e9b5912e4feb826f86eba5849cbd","7df631a1c8364c51a93422a2c4116e0f","36391b3d917149e1bb2357dd382cedbb","09e5693563094338a1a06f22cea202a0","9128a4798a0249f7b9acfb404b277190","3414947a3a0e403092050bdfb7e4d7ca","04bef2bd09cc436bb64d17c716b23a21","095934a18a3a4dbe8595ccfa8f54fedb","1a1dd862bfc146408557a065671af63b","07e2e431cb6840f49812e20bf4832532","acc858d7b15040ff925818726250ccd5","827714c3c4fb4eafac1377ac4adc975b","885e8b0d99174ef3bcff55b46b7d24a6","402043caafa343e389a127a748024a5a","486d50d223794b0a93f942fe668d0a3d","5e4564f92bb54cb397891e7d71d2b6d3"]},"id":"ezbEvFA9IA_K","outputId":"9bd72ed0-8eee-41a5-af32-49e838f95219","executionInfo":{"status":"error","timestamp":1678813893776,"user_tz":-330,"elapsed":21805,"user":{"displayName":"Pathirage Nipun Deelaka","userId":"00264959805100472870"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["================ Fold-0 ================\n","Unseen Classes : ['watching TV', 'house cleaning', 'standing', 'ascending stairs']\n","seen classes >  [0, 1, 3, 4, 5, 6, 8, 9, 11, 12, 13, 14, 16, 17]\n","unseen classes >  [7, 15, 2, 10]\n","Initiate IMU datasets ...\n"]},{"output_type":"display_data","data":{"text/plain":["Training Epoch:   0%|          | 0/15 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8601a6a245fd4f7eadc97b48d10cd54b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train:   0%|          | 0/588 [00:00<?, ?batch/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3414947a3a0e403092050bdfb7e4d7ca"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n","y_pred >  torch.Size([32, 400]) cls >  torch.Size([32]) selected_features >  torch.Size([14, 400])\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-29e7d526651c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseen_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mtrain_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mtrain_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phase'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-d7e9a7efd900>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, dataset, optimizer, loss_module, device, class_names, phase, l2_reg, loss_alpha)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m#loss = cross_entropy_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeat_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_alpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mclass_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mclass_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_selected_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-83a54da8cdf7>\u001b[0m in \u001b[0;36mpredict_class\u001b[0;34m(y_pred, selected_features)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mcosine_sim_comb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mcosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselected_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mcosine_sim_comb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_sim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"cpSPnALYIC-N"},"execution_count":null,"outputs":[]}]}