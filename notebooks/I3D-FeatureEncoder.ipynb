{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":996,"status":"ok","timestamp":1678732421284,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"iH7043iKpWSx","outputId":"376aea5e-0d7b-48eb-8005-69ef96b2977b"},"outputs":[{"name":"stdout","output_type":"stream","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2022 NVIDIA Corporation\n","Built on Wed_Sep_21_10:33:58_PDT_2022\n","Cuda compilation tools, release 11.8, V11.8.89\n","Build cuda_11.8.r11.8/compiler.31833905_0\n","gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n","Copyright (C) 2019 Free Software Foundation, Inc.\n","This is free software; see the source for copying conditions.  There is NO\n","warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n","\n"]}],"source":["# Check nvcc version\n","!nvcc -V\n","# Check GCC version\n","!gcc --version"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":208239,"status":"ok","timestamp":1678739286378,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"QtwAoRT-pYX-"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu111 (from versions: 1.11.0, 1.11.0+cpu, 1.11.0+cu102, 1.11.0+cu113, 1.11.0+cu115, 1.11.0+rocm4.3.1, 1.11.0+rocm4.5.2, 1.12.0, 1.12.0+cpu, 1.12.0+cu102, 1.12.0+cu113, 1.12.0+cu116, 1.12.0+rocm5.0, 1.12.0+rocm5.1.1, 1.12.1, 1.12.1+cpu, 1.12.1+cu102, 1.12.1+cu113, 1.12.1+cu116, 1.12.1+rocm5.0, 1.12.1+rocm5.1.1, 1.13.0, 1.13.0+cpu, 1.13.0+cu116, 1.13.0+cu117, 1.13.0+cu117.with.pypi.cudnn, 1.13.0+rocm5.1.1, 1.13.0+rocm5.2, 1.13.1, 1.13.1+cpu, 1.13.1+cu116, 1.13.1+cu117, 1.13.1+cu117.with.pypi.cudnn, 1.13.1+rocm5.1.1, 1.13.1+rocm5.2, 2.0.0, 2.0.0+cpu, 2.0.0+cpu.cxx11.abi, 2.0.0+cu117, 2.0.0+cu117.with.pypi.cudnn, 2.0.0+cu118, 2.0.0+rocm5.3, 2.0.0+rocm5.4.2, 2.0.1, 2.0.1+cpu, 2.0.1+cpu.cxx11.abi, 2.0.1+cu117, 2.0.1+cu117.with.pypi.cudnn, 2.0.1+cu118, 2.0.1+rocm5.3, 2.0.1+rocm5.4.2)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.9.0+cu111\u001b[0m\u001b[31m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html\n","Collecting mmcv-full\n","  Downloading mmcv-full-1.7.1.tar.gz (605 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m605.4/605.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting addict (from mmcv-full)\n","  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mmcv-full) (1.22.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mmcv-full) (23.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from mmcv-full) (8.4.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from mmcv-full) (6.0)\n","Collecting yapf (from mmcv-full)\n","  Downloading yapf-0.33.0-py2.py3-none-any.whl (200 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.9/200.9 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tomli\u003e=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf-\u003emmcv-full) (2.0.1)\n","Building wheels for collected packages: mmcv-full\n"]}],"source":["# install dependencies: (use cu111 because colab has CUDA 11.1)\n","!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n","\n","# install mmcv-full thus we could use CUDA operators\n","!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html\n","\n","# Install mmaction2\n","!rm -rf mmaction2\n","!git clone https://github.com/open-mmlab/mmaction2.git\n","%cd mmaction2\n","\n","!pip install -e .\n","\n","# Install some optional requirements\n","!pip install -r requirements/optional.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43204,"status":"ok","timestamp":1678739329570,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"aoc-Ei36iXmJ","outputId":"d15cadf3-8bc8-42a3-b01a-edb07bdde3eb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6105,"status":"ok","timestamp":1678739335671,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"BC6u24fkpaL8","outputId":"b03fcd3b-b0fd-40b7-d861-9bf900aeeb23"},"outputs":[{"name":"stdout","output_type":"stream","text":["1.9.0+cu111 True\n","0.24.1\n","11.1\n","GCC 7.3\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n","  warnings.warn(\n"]}],"source":["# Check Pytorch installation\n","import torch, torchvision\n","print(torch.__version__, torch.cuda.is_available())\n","\n","# Check MMAction2 installation\n","import mmaction\n","print(mmaction.__version__)\n","\n","# Check MMCV installation\n","from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n","print(get_compiling_cuda_version())\n","print(get_compiler_version())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2923,"status":"ok","timestamp":1678739338589,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"azzGoo7XpdF0","outputId":"7880388a-60d9-4b34-dbfd-1e78bf147248"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-03-13 20:28:55--  https://download.openmmlab.com/mmaction/recognition/tsn/tsn_r50_1x1x3_100e_kinetics400_rgb/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n","Resolving download.openmmlab.com (download.openmmlab.com)... 47.246.48.209, 47.246.48.211, 47.246.48.206, ...\n","Connecting to download.openmmlab.com (download.openmmlab.com)|47.246.48.209|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 97579339 (93M) [application/octet-stream]\n","Saving to: ‘checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth’\n","\n","checkpoints/tsn_r50 100%[===================\u003e]  93.06M  44.1MB/s    in 2.1s    \n","\n","2023-03-13 20:28:57 (44.1 MB/s) - ‘checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth’ saved [97579339/97579339]\n","\n"]}],"source":["!mkdir checkpoints\n","!wget -c https://download.openmmlab.com/mmaction/recognition/tsn/tsn_r50_1x1x3_100e_kinetics400_rgb/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth \\\n","      -O checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12483,"status":"ok","timestamp":1678739351069,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"s2er4P2npfo5","outputId":"72a1c903-9179-4306-c79e-0c4cbaddb524"},"outputs":[{"name":"stdout","output_type":"stream","text":["load checkpoint from local path: checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n"]}],"source":["from mmaction.apis import init_recognizer\n","\n","# Choose to use a config and initialize the recognizer\n","config = 'configs/recognition/tsn/tsn_r50_video_inference_1x1x3_100e_kinetics400_rgb.py'\n","# Setup a checkpoint file to load\n","checkpoint = 'checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n","# Initialize the recognizer\n","model = init_recognizer(config, checkpoint, device='cuda:0')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1678739351069,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"vs4LW_noJF_Q","outputId":"60294c07-c131-42d9-c1d8-b78d64cd935f"},"outputs":[{"data":{"text/plain":["Recognizer2D(\n","  (backbone): ResNet(\n","    (conv1): ConvModule(\n","      (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (activate): ReLU(inplace=True)\n","    )\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): Bottleneck(\n","        (conv1): ConvModule(\n","          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv2): ConvModule(\n","          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv3): ConvModule(\n","          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (relu): ReLU(inplace=True)\n","        (downsample): ConvModule(\n","          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): ConvModule(\n","          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv2): ConvModule(\n","          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv3): ConvModule(\n","          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): ConvModule(\n","          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv2): ConvModule(\n","          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv3): ConvModule(\n","          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): Bottleneck(\n","        (conv1): ConvModule(\n","          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv2): ConvModule(\n","          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv3): ConvModule(\n","          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (relu): ReLU(inplace=True)\n","        (downsample): ConvModule(\n","          (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): ConvModule(\n","          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv2): ConvModule(\n","          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv3): ConvModule(\n","          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): ConvModule(\n","          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv2): ConvModule(\n","          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv3): ConvModule(\n","          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): ConvModule(\n","          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv2): ConvModule(\n","          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv3): ConvModule(\n","          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): Bottleneck(\n","        (conv1): ConvModule(\n","          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv2): ConvModule(\n","          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv3): ConvModule(\n","          (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (relu): ReLU(inplace=True)\n","        (downsample): ConvModule(\n","          (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): ConvModule(\n","          (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv2): ConvModule(\n","          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv3): ConvModule(\n","          (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): ConvModule(\n","          (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv2): ConvModule(\n","          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv3): ConvModule(\n","          (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): ConvModule(\n","          (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv2): ConvModule(\n","          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv3): ConvModule(\n","          (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (relu): ReLU(inplace=True)\n","      )\n","      (4): Bottleneck(\n","        (conv1): ConvModule(\n","          (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv2): ConvModule(\n","          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv3): ConvModule(\n","          (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (relu): ReLU(inplace=True)\n","      )\n","      (5): Bottleneck(\n","        (conv1): ConvModule(\n","          (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv2): ConvModule(\n","          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv3): ConvModule(\n","          (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): Bottleneck(\n","        (conv1): ConvModule(\n","          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv2): ConvModule(\n","          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv3): ConvModule(\n","          (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (relu): ReLU(inplace=True)\n","        (downsample): ConvModule(\n","          (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): ConvModule(\n","          (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv2): ConvModule(\n","          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv3): ConvModule(\n","          (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): ConvModule(\n","          (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv2): ConvModule(\n","          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (activate): ReLU(inplace=True)\n","        )\n","        (conv3): ConvModule(\n","          (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","  )\n","  (cls_head): TSNHead(\n","    (loss_cls): CrossEntropyLoss()\n","    (consensus): AvgConsensus()\n","    (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (dropout): Dropout(p=0.4, inplace=False)\n","    (fc_cls): Linear(in_features=2048, out_features=400, bias=True)\n","  )\n",")"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1678739351069,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"pfvi2SPiJF2E","outputId":"6e35624f-f9f9-494c-a5b0-fb2de7244ce3"},"outputs":[{"data":{"text/plain":["Linear(in_features=2048, out_features=400, bias=True)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["model.cls_head.fc_cls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_JKfEgHlLmDo"},"outputs":[],"source":["import os\n","import os.path as osp\n","import re\n","import warnings\n","from operator import itemgetter\n","\n","import mmcv\n","import numpy as np\n","import torch\n","from mmcv.parallel import collate, scatter\n","from mmcv.runner import load_checkpoint\n","\n","from mmaction.core import OutputHook\n","from mmaction.datasets.pipelines import Compose\n","from mmaction.models import build_recognizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s55EEbjSLl5Y"},"outputs":[],"source":["def inference_recognizer(model, video, outputs=None, as_tensor=True, **kwargs):\n","    \"\"\"Inference a video with the recognizer.\n","\n","    Args:\n","        model (nn.Module): The loaded recognizer.\n","        video (str | dict | ndarray): The video file path / url or the\n","            rawframes directory path / results dictionary (the input of\n","            pipeline) / a 4D array T x H x W x 3 (The input video).\n","        outputs (list(str) | tuple(str) | str | None) : Names of layers whose\n","            outputs need to be returned, default: None.\n","        as_tensor (bool): Same as that in ``OutputHook``. Default: True.\n","\n","    Returns:\n","        dict[tuple(str, float)]: Top-5 recognition result dict.\n","        dict[torch.tensor | np.ndarray]:\n","            Output feature maps from layers specified in `outputs`.\n","    \"\"\"\n","    if 'use_frames' in kwargs:\n","        warnings.warn('The argument `use_frames` is deprecated PR #1191. '\n","                      'Now you can use models trained with frames or videos '\n","                      'arbitrarily. ')\n","    if 'label_path' in kwargs:\n","        warnings.warn('The argument `use_frames` is deprecated PR #1191. '\n","                      'Now the label file is not needed in '\n","                      'inference_recognizer. ')\n","\n","    input_flag = None\n","    if isinstance(video, dict):\n","        input_flag = 'dict'\n","    elif isinstance(video, np.ndarray):\n","        assert len(video.shape) == 4, 'The shape should be T x H x W x C'\n","        input_flag = 'array'\n","    elif isinstance(video, str) and video.startswith('http'):\n","        input_flag = 'video'\n","    elif isinstance(video, str) and osp.exists(video):\n","        if osp.isfile(video):\n","            if video.endswith('.npy'):\n","                input_flag = 'audio'\n","            else:\n","                input_flag = 'video'\n","        if osp.isdir(video):\n","            input_flag = 'rawframes'\n","    else:\n","        raise RuntimeError('The type of argument video is not supported: '\n","                           f'{type(video)}')\n","\n","    if isinstance(outputs, str):\n","        outputs = (outputs, )\n","    assert outputs is None or isinstance(outputs, (tuple, list))\n","\n","    cfg = model.cfg\n","    device = next(model.parameters()).device  # model device\n","    # build the data pipeline\n","    test_pipeline = cfg.data.test.pipeline\n","    # Alter data pipelines \u0026 prepare inputs\n","    if input_flag == 'dict':\n","        data = video\n","    if input_flag == 'array':\n","        modality_map = {2: 'Flow', 3: 'RGB'}\n","        modality = modality_map.get(video.shape[-1])\n","        data = dict(\n","            total_frames=video.shape[0],\n","            label=-1,\n","            start_index=0,\n","            array=video,\n","            modality=modality)\n","        for i in range(len(test_pipeline)):\n","            if 'Decode' in test_pipeline[i]['type']:\n","                test_pipeline[i] = dict(type='ArrayDecode')\n","        test_pipeline = [x for x in test_pipeline if 'Init' not in x['type']]\n","    if input_flag == 'video':\n","        data = dict(filename=video, label=-1, start_index=0, modality='RGB')\n","        if 'Init' not in test_pipeline[0]['type']:\n","            test_pipeline = [dict(type='OpenCVInit')] + test_pipeline\n","        else:\n","            test_pipeline[0] = dict(type='OpenCVInit')\n","        for i in range(len(test_pipeline)):\n","            if 'Decode' in test_pipeline[i]['type']:\n","                test_pipeline[i] = dict(type='OpenCVDecode')\n","    if input_flag == 'rawframes':\n","        filename_tmpl = cfg.data.test.get('filename_tmpl', 'img_{:05}.jpg')\n","        modality = cfg.data.test.get('modality', 'RGB')\n","        start_index = cfg.data.test.get('start_index', 1)\n","\n","        # count the number of frames that match the format of `filename_tmpl`\n","        # RGB pattern example: img_{:05}.jpg -\u003e ^img_\\d+.jpg$\n","        # Flow patteren example: {}_{:05d}.jpg -\u003e ^x_\\d+.jpg$\n","        pattern = f'^{filename_tmpl}$'\n","        if modality == 'Flow':\n","            pattern = pattern.replace('{}', 'x')\n","        pattern = pattern.replace(\n","            pattern[pattern.find('{'):pattern.find('}') + 1], '\\\\d+')\n","        total_frames = len(\n","            list(\n","                filter(lambda x: re.match(pattern, x) is not None,\n","                       os.listdir(video))))\n","        data = dict(\n","            frame_dir=video,\n","            total_frames=total_frames,\n","            label=-1,\n","            start_index=start_index,\n","            filename_tmpl=filename_tmpl,\n","            modality=modality)\n","        if 'Init' in test_pipeline[0]['type']:\n","            test_pipeline = test_pipeline[1:]\n","        for i in range(len(test_pipeline)):\n","            if 'Decode' in test_pipeline[i]['type']:\n","                test_pipeline[i] = dict(type='RawFrameDecode')\n","    if input_flag == 'audio':\n","        data = dict(\n","            audio_path=video,\n","            total_frames=len(np.load(video)),\n","            start_index=cfg.data.test.get('start_index', 1),\n","            label=-1)\n","\n","    test_pipeline = Compose(test_pipeline)\n","    data = test_pipeline(data)\n","    data = collate([data], samples_per_gpu=1)\n","\n","    if next(model.parameters()).is_cuda:\n","        # scatter to specified GPU\n","        data = scatter(data, [device])[0]\n","\n","    # forward the model\n","    # with OutputHook(model, outputs=outputs, as_tensor=as_tensor) as h:\n","    # model.cls_head.register_forward_hook(get_activation('dropout'))\n","    # with torch.no_grad():\n","    #     result = model(return_loss=False, **data)\n","    with torch.no_grad():\n","        my_output = {}\n","        \n","        def my_2048_hook(module_, input_, output_):\n","            nonlocal my_output\n","            my_output[\"2048\"] = input_\n","\n","        def my_400_hook(module_,input_,output_):\n","            nonlocal my_output\n","            my_output[\"400\"] = output_\n","\n","        #a_hook = model.cls_head.dropout.register_forward_hook(my_2048_hook)       \n","        a_hook = model.cls_head.fc_cls.register_forward_hook(my_2048_hook)\n","        a_hook = model.cls_head.fc_cls.register_forward_hook(my_400_hook)  \n","        model(return_loss=False, **data)\n","        a_hook.remove()\n","        # return my_output\n","    #     returned_features = h.layer_outputs if outputs else None\n","\n","    # num_classes = scores.shape[-1]\n","    # score_tuples = tuple(zip(range(num_classes), scores))\n","    # score_sorted = sorted(score_tuples, key=itemgetter(1), reverse=True)\n","\n","    # top5_label = score_sorted[:5]\n","    # if outputs:\n","    #     return top5_label, returned_features\n","    return my_output[\"2048\"][0].squeeze(),my_output[\"400\"].squeeze()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6057,"status":"ok","timestamp":1678739357123,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"6HTnYMeCphzh","outputId":"eb6b286f-b7ca-46a7-e58f-a44c636691a1"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]}],"source":["# Use the recognizer to do inference\n","video = '/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Datasets/hand_picked_videos/stand/SoundAndTheStory_stand_f_cm_np1_ri_med_0.avi'\n","#label = '/content/mmaction2/tools/data/kinetics/label_map_k400.txt'\n","results = inference_recognizer(model, video)\n","\n","# print(results)\n","# labels = open(label).readlines()\n","# labels = [x.strip() for x in labels]\n","# results = [(labels[k[0]], k[1]) for k in results]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1678739357123,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"erxGMdl-OZ09","outputId":"7014a4f9-b13c-48f2-bae2-5ee1d18cf825"},"outputs":[{"data":{"text/plain":["(torch.Tensor, torch.Size([2048]), torch.Tensor, torch.Size([400]))"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["type(results[0]), results[0].shape,type(results[1]), results[1].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1678739357123,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"L3bJc-ujpiEc","outputId":"6377f191-2fc2-427f-e33f-143cafaba605"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.27062904834747314:  tensor(0.3986, device='cuda:0')\n","1.495266079902649:  tensor(3.3786, device='cuda:0')\n"]}],"source":["# Let's show the results\n","for result in results:\n","    print(f'{result[0]}: ', result[1])"]},{"cell_type":"markdown","metadata":{"id":"t2NRXKdlPqsM"},"source":["---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C1CdnlJVRNzk"},"outputs":[],"source":["import os \n","import glob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D64mw7BkQW1Z"},"outputs":[],"source":["dir_path = '/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Datasets/Consolidated/PAMPA2/Videos/'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81},"executionInfo":{"elapsed":2709534,"status":"ok","timestamp":1678742066655,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"4GeTO-MvShBa","outputId":"6ba5a108-6473-469a-c18e-afbb56f25fd6"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"92c1dcb433aa4e41a6e1711fd0fcc01d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/224 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66bbbd50ba6c4e2a83e1924740495e97","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from tqdm.autonotebook import tqdm\n","\n","video_ft_dict = {}\n","for p in tqdm(glob.glob(dir_path+'*/*.mp4',recursive=True)):\n","  #action = p.split('/')[-1].split(\"_\")[0]\n","  action = p.split('/')[-2]\n","  outputs = inference_recognizer(model, p)\n","  \n","  ft_vector = []\n","\n","  for i in range(len(outputs)):\n","    ft_vector.append(outputs[i].cpu().numpy())\n","\n","  try:\n","    video_ft_dict[action].append(ft_vector)\n","  except:\n","    video_ft_dict[action] = [ft_vector]\n","    \n","for p in tqdm(glob.glob(dir_path+'*/*.avi',recursive=True)):\n","  #action = p.split('/')[-1].split(\"_\")[0]\n","  action = p.split('/')[-2]\n","  outputs = inference_recognizer(model, p)\n","  \n","  ft_vector = []\n","\n","  for i in range(len(outputs)):\n","    ft_vector.append(outputs[i].cpu().numpy())\n","\n","  try:\n","    video_ft_dict[action].append(ft_vector)\n","  except:\n","    video_ft_dict[action] = [ft_vector]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1678742066656,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"3jydh_o8Yr3E","outputId":"b20c34b6-8963-4371-f871-0bbea52df1a7"},"outputs":[{"data":{"text/plain":["dict_keys(['Nordic walking', 'running', 'ironing', 'ascending stairs', 'vacuum cleaning', 'walking', 'watching TV', 'computer work', 'playing soccer', 'car driving', 'house cleaning', 'standing', 'folding laundry', 'rope jumping', 'lying', 'sitting', 'descending stairs', 'cycling'])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["video_ft_dict.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ma7c7XGjtJQJ"},"outputs":[],"source":["action_map = {\"lying_on_floor\": \"lying\",\n","              \"sitting_down\": \"sitting\",\n","              \"stand\": \"standing\",\n","              \"walk\": \"walking\",\n","              \"run\": \"running\",\n","              \"ride_bike\": \"cycling\",\n","              \"nordic_walk\": \"Nordic walking\",\n","              \"watching_TV\": \"watching TV\",\n","              \"using_computer\": \"computer work\",\n","              \"car_drive\": \"car driving\",\n","              \"going_up_stairs\": \"ascending stairs\",\n","              \"going_down_stairs\": \"descending stairs\",\n","              \"vacuum_cleaning\": \"vacuum cleaning\",\n","              \"ironing\": \"ironing\",\n","              \"folding_laundry\": \"folding laundry\",\n","              \"house_cleaning\": \"house cleaning\",\n","              \"kick_ball\": \"playing soccer\",\n","              \"jump\": \"rope jumping\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ILS4Rm5SYYUU"},"outputs":[],"source":["feat_dict = {action_map[k]: v for k,v in video_ft_dict.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rWRXeXeV8GPn"},"outputs":[],"source":["feat_dict = video_ft_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9oIa6b7R5PXC"},"outputs":[],"source":["os.makedirs(\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Datasets/Consolidated/PAMPA2/I3D\",exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eWsdny-IYoJl"},"outputs":[],"source":["import pickle \n","\n","with open('/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Datasets/Consolidated/PAMPA2/I3D/video_feat.pkl', 'wb') as f:\n","    pickle.dump(feat_dict, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oLZsOk0tZFpe"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"02e5fd7895df41eb82d7f8e84cff7d87":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0fc086b64c80406999f0e9f49bad37c1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b857565994f4cdda5171dacf91b0f07":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b331fcc03e6542afb846b76760711362","max":224,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d6a2deb897844cafacde1a4cb964855f","value":224}},"51439ec0bd9b4ef88b5b072f6c511859":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb7f6c562a974acf978b8b0a7667c5bc","placeholder":"​","style":"IPY_MODEL_b3758f1664bc4301af3adc906aef9613","value":" 0/0 [00:00\u0026lt;?, ?it/s]"}},"51dcbd4eb4c84ffbbd42c89ea9a2d040":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66bbbd50ba6c4e2a83e1924740495e97":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bdd87d3e99934410aba01a743ab93419","IPY_MODEL_c731c6e98b954321af0dbe912e7ab582","IPY_MODEL_51439ec0bd9b4ef88b5b072f6c511859"],"layout":"IPY_MODEL_02e5fd7895df41eb82d7f8e84cff7d87"}},"7233fdbb4c3a445da069a620d57fcbca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"737f580e35394644a54a589a7d02ccba":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"793fa120eca34a76bb67cb7ca303d87d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81c14115bd0c464995076e397ab7429b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"92c1dcb433aa4e41a6e1711fd0fcc01d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c889387700fb4abba5df61bdcd26c739","IPY_MODEL_3b857565994f4cdda5171dacf91b0f07","IPY_MODEL_ea835fe9d5f54756a73638c9717e4ab5"],"layout":"IPY_MODEL_f40f23ca34654d0b90a50e3abe4ab846"}},"b331fcc03e6542afb846b76760711362":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3758f1664bc4301af3adc906aef9613":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bdd87d3e99934410aba01a743ab93419":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fc086b64c80406999f0e9f49bad37c1","placeholder":"​","style":"IPY_MODEL_81c14115bd0c464995076e397ab7429b","value":""}},"c5f63223fcb34a51ac5315aeae0ec756":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c731c6e98b954321af0dbe912e7ab582":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f481841f7b3b444baaa9f1f40d278c38","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c5f63223fcb34a51ac5315aeae0ec756","value":0}},"c889387700fb4abba5df61bdcd26c739":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_737f580e35394644a54a589a7d02ccba","placeholder":"​","style":"IPY_MODEL_7233fdbb4c3a445da069a620d57fcbca","value":"100%"}},"cb7f6c562a974acf978b8b0a7667c5bc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6a2deb897844cafacde1a4cb964855f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea835fe9d5f54756a73638c9717e4ab5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51dcbd4eb4c84ffbbd42c89ea9a2d040","placeholder":"​","style":"IPY_MODEL_793fa120eca34a76bb67cb7ca303d87d","value":" 224/224 [45:05\u0026lt;00:00, 21.81s/it]"}},"f40f23ca34654d0b90a50e3abe4ab846":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f481841f7b3b444baaa9f1f40d278c38":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}}}}},"nbformat":4,"nbformat_minor":0}