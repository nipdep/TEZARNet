{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import MultiheadAttention, Linear, Dropout, BatchNorm1d, TransformerEncoderLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeGlobalAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, max_len=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        d_head, remainder = divmod(d_model, num_heads)\n",
    "        if remainder:\n",
    "            raise ValueError(\n",
    "                \"incompatible `d_model` and `num_heads`\"\n",
    "            )\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.Er = nn.Parameter(torch.randn(max_len, d_head))\n",
    "        self.register_buffer(\n",
    "            \"mask\", \n",
    "            torch.tril(torch.ones(max_len, max_len))\n",
    "            .unsqueeze(0).unsqueeze(0)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        if seq_len > self.max_len:\n",
    "            raise ValueError(\n",
    "                \"sequence length exceeds model capacity\"\n",
    "            )\n",
    "        \n",
    "        k_t = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
    "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "        \n",
    "        start = self.max_len - seq_len\n",
    "        Er_t = self.Er[start:, :].transpose(0, 1) # automatic positional padding\n",
    "        QEr = torch.matmul(q, Er_t)\n",
    "        Srel = self.skew(QEr)\n",
    "        \n",
    "        QK_t = torch.matmul(q, k_t)\n",
    "        attn = (QK_t + Srel) / math.sqrt(q.size(-1))\n",
    "        mask = self.mask[:, :, :seq_len, :seq_len]\n",
    "        attn = attn.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2)\n",
    "        out = out.reshape(batch_size, seq_len, -1)\n",
    "        return self.dropout(out)\n",
    "        \n",
    "    \n",
    "    def skew(self, QEr):\n",
    "        padded = F.pad(QEr, (1, 0))\n",
    "        batch_size, num_heads, num_rows, num_cols = padded.shape\n",
    "        reshaped = padded.reshape(batch_size, num_heads, num_cols, num_rows)\n",
    "        Srel = reshaped[:, :, 1:, :]\n",
    "        return Srel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZSLHARNet(nn.Module):\n",
    "    def __init__(self, in_ft, d_model, num_heads, ft_size, attr_size, max_len=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_ft = in_ft\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ft_size = ft_size # semantic space size <-> output feature space size\n",
    "        self.attr_size = attr_size # intermediate attribute space size \n",
    "\n",
    "        # custom sample layer configuration\n",
    "        # Dense layer for feature projection\n",
    "        self.DenseL = nn.Linear(in_ft, d_model)\n",
    "        # attention encoder <-> global relative attention used here\n",
    "        self.AttnL = RelativeGlobalAttention(d_model, num_heads, max_len)\n",
    "        # positional encoding concat <-> LSTM \n",
    "        self.lstmL = nn.LSTM(input_size=d_model, hidden_size=ft_size, batch_first=True)\n",
    "        # SAE submodule\n",
    "        self.EncDenseL = nn.Linear(in_features=ft_size, out_features=attr_size, bias=False)\n",
    "        self.DecDenseL = nn.Linear(in_features=attr_size, out_features=ft_size, bias=False)\n",
    "        # override weights\n",
    "        del self.EncDenseL.weight\n",
    "        del self.DecDenseL.weight\n",
    "        # define shared weights\n",
    "        self.TransMet = nn.Parameter(torch.randn(attr_size, ft_size))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.DenseL(x)\n",
    "        out = self.AttnL(out)\n",
    "        lstm_out, hidden = self.lstmL(out)\n",
    "        # SAE Operation\n",
    "        self.EncDenseL.weight = self.TransMet\n",
    "        self.DecDenseL.weight = self.TransMet.T \n",
    "        attr_out = self.EncDenseL(lstm_out[:, -1, :])\n",
    "        ft_out = self.DecDenseL(attr_out)\n",
    "        return attr_out, ft_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    raise ValueError(\"activation should be relu/gelu, not {}\".format(activation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedPositionalEncoding(nn.Module): # deterministic positional encoding\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
    "        in the sequence. The positional encodings have the same dimension as\n",
    "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
    "        functions of different frequencies.\n",
    "    .. math::\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=1024).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1024, scale_factor=1.0):\n",
    "        super(FixedPositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)  # positional encoding\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = scale_factor * pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)  # this stores the variable in the state_dict (used for non-trainable variables)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnablePositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1024):\n",
    "        super(LearnablePositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # Each position gets its own embedding\n",
    "        # Since indices are always 0 ... max_len, we don't have to do a look-up\n",
    "        self.pe = nn.Parameter(torch.empty(max_len, 1, d_model))  # requires_grad automatically set to True\n",
    "        nn.init.uniform_(self.pe, -0.02, 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "def get_pos_encoder(pos_encoding):\n",
    "    if pos_encoding == \"learnable\":\n",
    "        return LearnablePositionalEncoding\n",
    "    elif pos_encoding == \"fixed\":\n",
    "        return FixedPositionalEncoding\n",
    "\n",
    "    raise NotImplementedError(\"pos_encoding should be 'learnable'/'fixed', not '{}'\".format(pos_encoding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBatchNormEncoderLayer(nn.modules.Module):\n",
    "    r\"\"\"This transformer encoder layer block is made up of self-attn and feedforward network.\n",
    "    It differs from TransformerEncoderLayer in torch/nn/modules/transformer.py in that it replaces LayerNorm\n",
    "    with BatchNorm.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerBatchNormEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = BatchNorm1d(d_model, eps=1e-5)  # normalizes each feature across batch samples and time steps\n",
    "        self.norm2 = BatchNorm1d(d_model, eps=1e-5)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerBatchNormEncoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)  # (seq_len, batch_size, d_model)\n",
    "        src = src.permute(1, 2, 0)  # (batch_size, d_model, seq_len)\n",
    "        # src = src.reshape([src.shape[0], -1])  # (batch_size, seq_length * d_model)\n",
    "        src = self.norm1(src)\n",
    "        src = src.permute(2, 0, 1)  # restore (seq_len, batch_size, d_model)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)  # (seq_len, batch_size, d_model)\n",
    "        src = src.permute(1, 2, 0)  # (batch_size, d_model, seq_len)\n",
    "        src = self.norm2(src)\n",
    "        src = src.permute(2, 0, 1)  # restore (seq_len, batch_size, d_model)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSTransformerEncoderClassiregressor(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplest classifier/regressor. Can be either regressor or classifier because the output does not include\n",
    "    softmax. Concatenates final layer embeddings and uses 0s to ignore padding embeddings in final output layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feat_dim, max_len, d_model, n_heads, num_layers, dim_feedforward, ft_size,\n",
    "                 dropout=0.1, pos_encoding='fixed', activation='gelu', norm='BatchNorm', freeze=False):\n",
    "        super(TSTransformerEncoderClassiregressor, self).__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.project_inp = nn.Linear(feat_dim, d_model)\n",
    "        self.pos_enc = get_pos_encoder(pos_encoding)(d_model, dropout=dropout*(1.0 - freeze), max_len=max_len)\n",
    "\n",
    "        if norm == 'LayerNorm':\n",
    "            encoder_layer = TransformerEncoderLayer(d_model, self.n_heads, dim_feedforward, dropout*(1.0 - freeze), activation=activation)\n",
    "        else:\n",
    "            encoder_layer = TransformerBatchNormEncoderLayer(d_model, self.n_heads, dim_feedforward, dropout*(1.0 - freeze), activation=activation)\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        self.act = _get_activation_fn(activation)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.feat_dim = feat_dim\n",
    "        self.ft_size = ft_size\n",
    "        self.conv1d = nn.Conv1d(self.max_len, 1, 1)\n",
    "        self.dense1 = nn.Linear(128, 128)\n",
    "        self.output_layer = self.build_output_module(d_model, max_len, ft_size)\n",
    "\n",
    "    def build_output_module(self, d_model, max_len, num_classes):\n",
    "        # output_layer = nn.Linear(d_model * max_len, num_classes)\n",
    "        output_layer = nn.Linear(d_model, num_classes)\n",
    "        # no softmax (or log softmax), because CrossEntropyLoss does this internally. If probabilities are needed,\n",
    "        # add F.log_softmax and use NLLoss\n",
    "        return output_layer\n",
    "\n",
    "    def forward(self, X, padding_masks=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: (batch_size, seq_length, feat_dim) torch tensor of masked features (input)\n",
    "            padding_masks: (batch_size, seq_length) boolean tensor, 1 means keep vector at this position, 0 means padding\n",
    "        Returns:\n",
    "            output: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        # permute because pytorch convention for transformers is [seq_length, batch_size, feat_dim]. padding_masks [batch_size, feat_dim]\n",
    "        inp = X.permute(1, 0, 2)\n",
    "        inp = self.project_inp(inp) * math.sqrt(self.d_model)  # [seq_length, batch_size, d_model] project input vectors to d_model dimensional space\n",
    "        inp = self.pos_enc(inp)  # add positional encoding\n",
    "        # NOTE: logic for padding masks is reversed to comply with definition in MultiHeadAttention, TransformerEncoderLayer\n",
    "        # output = self.transformer_encoder(inp, src_key_padding_mask=~padding_masks)  # (seq_length, batch_size, d_model) #@nipdep\n",
    "        if padding_masks:\n",
    "            output = self.transformer_encoder(inp, src_key_padding_mask=~padding_masks)  # (seq_length, batch_size, d_model)\n",
    "        else:\n",
    "            output = self.transformer_encoder(inp)  # (seq_length, batch_size, d_model)\n",
    "        output = self.act(output)  # the output transformer encoder/decoder embeddings don't include non-linearity\n",
    "        output = output.permute(1, 0, 2)  # (batch_size, seq_length, d_model)\n",
    "        # output = self.dropout1(output)\n",
    "\n",
    "        # Output\n",
    "        # output = output * padding_masks.unsqueeze(-1)  # zero-out padding embeddings #@nipdep\n",
    "        # output = output.reshape(output.shape[0], -1)  # (batch_size, seq_length * d_model)\n",
    "        # output = output.permute(0, 2, 1)\n",
    "        # output = self.conv1d(output)\n",
    "        output = nn.AvgPool2d((self.max_len,1))(output)\n",
    "        output = torch.squeeze(output)\n",
    "        output = nn.Dropout1d(0.1)(output)\n",
    "        output = self.dense1(output)\n",
    "        output = self.output_layer(output)  # (batch_size, num_classes)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Util Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeGlobalAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, max_len=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        d_head, remainder = divmod(d_model, num_heads)\n",
    "        if remainder:\n",
    "            raise ValueError(\n",
    "                \"incompatible `d_model` and `num_heads`\"\n",
    "            )\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.Er = nn.Parameter(torch.randn(max_len, d_head))\n",
    "        self.register_buffer(\n",
    "            \"mask\", \n",
    "            torch.tril(torch.ones(max_len, max_len))\n",
    "            .unsqueeze(0).unsqueeze(0)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        if seq_len > self.max_len:\n",
    "            raise ValueError(\n",
    "                \"sequence length exceeds model capacity\"\n",
    "            )\n",
    "        \n",
    "        k_t = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
    "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "        \n",
    "        start = self.max_len - seq_len\n",
    "        Er_t = self.Er[start:, :].transpose(0, 1) # automatic positional padding\n",
    "        QEr = torch.matmul(q, Er_t)\n",
    "        Srel = self.skew(QEr)\n",
    "        \n",
    "        QK_t = torch.matmul(q, k_t)\n",
    "        attn = (QK_t + Srel) / math.sqrt(q.size(-1))\n",
    "        mask = self.mask[:, :, :seq_len, :seq_len]\n",
    "        attn = attn.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2)\n",
    "        out = out.reshape(batch_size, seq_len, -1)\n",
    "        return self.dropout(out)\n",
    "        \n",
    "    \n",
    "    def skew(self, QEr):\n",
    "        padded = F.pad(QEr, (1, 0))\n",
    "        batch_size, num_heads, num_rows, num_cols = padded.shape\n",
    "        reshaped = padded.reshape(batch_size, num_heads, num_cols, num_rows)\n",
    "        Srel = reshaped[:, :, 1:, :]\n",
    "        return Srel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Rel.Glob.Attn. > LSTM > SAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self, in_ft, d_model, num_heads, ft_size, attr_size, add_prepL=False, max_len=1024, dropout=0.1):\n",
    "        super(Model1, self).__init__()\n",
    "        self.add_prepL = add_prepL\n",
    "        self.in_ft = in_ft\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ft_size = ft_size \n",
    "        self.attr_size = attr_size\n",
    "\n",
    "        # feature prep layer\n",
    "        if self.add_prepL:\n",
    "            self.DenseL = nn.Linear(in_ft, d_model)\n",
    "        else:\n",
    "            self.d_model = self.in_ft \n",
    "        # relative global attention layer\n",
    "        self.AttnL = RelativeGlobalAttention(self.d_model, self.num_heads, self.max_len)\n",
    "        # positional encoding concat <-> LSTM \n",
    "        self.lstmL = nn.LSTM(input_size=d_model, hidden_size=ft_size, batch_first=True)\n",
    "        # SAE submodule\n",
    "        self.EncDenseL = nn.Linear(in_features=ft_size, out_features=attr_size, bias=False)\n",
    "        self.DecDenseL = nn.Linear(in_features=attr_size, out_features=ft_size, bias=False)\n",
    "        # override weights\n",
    "        del self.EncDenseL.weight\n",
    "        del self.DecDenseL.weight\n",
    "        # define shared weights\n",
    "        self.TransMet = nn.Parameter(torch.randn(attr_size, ft_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.add_prepL:\n",
    "            x = self.DenseL(x)\n",
    "\n",
    "        out = self.AttnL(x)\n",
    "        lstm_out, hidden = self.lstmL(out)\n",
    "        # SAE Operation\n",
    "        self.EncDenseL.weight = self.TransMet\n",
    "        self.DecDenseL.weight = self.TransMet.T \n",
    "        attr_out = self.EncDenseL(lstm_out[:, -1, :])\n",
    "        ft_out = self.DecDenseL(attr_out)\n",
    "        return attr_out, ft_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 31]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model \n",
    "sample_input = torch.randn((32, 120, 42))\n",
    "model = Model1(in_ft=42, d_model=124, num_heads=2, ft_size=64, attr_size=31, add_prepL=True, max_len=120, dropout=0.1)\n",
    "sample_output = model(sample_input)\n",
    "sample_output[0].shape, sample_output[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Rel.Glob.Attn. > LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, hidden_layers=64):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        # lstm1, lstm2, linear are all layers in the network\n",
    "        self.lstm1 = nn.LSTMCell(1, self.hidden_layers)\n",
    "        self.lstm2 = nn.LSTMCell(self.hidden_layers, self.hidden_layers)\n",
    "        self.linear = nn.Linear(self.hidden_layers, 1)\n",
    "        \n",
    "    def forward(self, y, future_preds=0):\n",
    "        outputs, num_samples = [], y.size(0)\n",
    "        h_t = torch.zeros(num_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        c_t = torch.zeros(num_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        h_t2 = torch.zeros(num_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        c_t2 = torch.zeros(num_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        \n",
    "        for time_step in y.split(1, dim=1):\n",
    "            # N, 1\n",
    "            h_t, c_t = self.lstm1(input_t, (h_t, c_t)) # initial hidden and cell states\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2)) # new hidden and cell states\n",
    "            output = self.linear(h_t2) # output from the last FC layer\n",
    "            outputs.append(output)\n",
    "            \n",
    "        for i in range(future_preds):\n",
    "            # this only generates future predictions if we pass in future_preds>0\n",
    "            # mirrors the code above, using last output/prediction as input\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs.append(output)\n",
    "        # transform list to tensor    \n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self, in_ft, d_model, num_heads, ft_size, add_prepL=False, max_len=1024, dropout=0.1):\n",
    "        super(Model2, self).__init__()\n",
    "        self.add_prepL = add_prepL\n",
    "        self.in_ft = in_ft\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ft_size = ft_size \n",
    "\n",
    "        # feature prep layer\n",
    "        if self.add_prepL:\n",
    "            self.DenseL = nn.Linear(in_ft, d_model)\n",
    "        else:\n",
    "            self.d_model = self.in_ft \n",
    "        # relative global attention layer\n",
    "        self.AttnL = RelativeGlobalAttention(self.d_model, self.num_heads, self.max_len)\n",
    "        # positional encoding concat <-> LSTM \n",
    "        self.lstmL = nn.LSTM(input_size=d_model, hidden_size=ft_size, batch_first=True)\n",
    "        # SAE submodule\n",
    "        self.DensePostL = nn.Linear(in_features=ft_size, out_features=ft_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.add_prepL:\n",
    "            x = self.DenseL(x)\n",
    "\n",
    "        out = self.AttnL(x)\n",
    "        lstm_out, hidden = self.lstmL(out)\n",
    "        out = self.DensePostL(lstm_out[:, -1, :])\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model \n",
    "sample_input = torch.randn((32, 120, 42))\n",
    "model = Model2(in_ft=42, d_model=128, num_heads=2, ft_size=64, add_prepL=True, max_len=120, dropout=0.1)\n",
    "sample_output = model(sample_input)\n",
    "sample_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Rel.Glob.Attn. > 1DConv > SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3(nn.Module):\n",
    "    def __init__(self, in_ft, d_model, num_heads, ft_size, attr_size, max_len=1024, dropout=0.1):\n",
    "        super(Model3, self).__init__()\n",
    "        self.in_ft = in_ft\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ft_size = ft_size \n",
    "\n",
    "        # feature prep layer\n",
    "        self.DenseL = nn.Linear(in_ft, d_model)\n",
    "        # relative global attention layer\n",
    "        self.AttnL = RelativeGlobalAttention(self.d_model, self.num_heads, self.max_len)\n",
    "        # positional encoding concat <-> 1DConv \n",
    "        self.Act = F.relu#_get_activation_fn(activation)\n",
    "        self.Conv1dL = nn.Conv1d(self.max_len, 1, 1)\n",
    "        self.DenseL2 = nn.Linear(self.d_model, self.ft_size)\n",
    "        # SAE submodule\n",
    "        self.EncDenseL = nn.Linear(in_features=ft_size, out_features=attr_size, bias=False)\n",
    "        self.DecDenseL = nn.Linear(in_features=attr_size, out_features=ft_size, bias=False)\n",
    "        # override weights\n",
    "        del self.EncDenseL.weight\n",
    "        del self.DecDenseL.weight\n",
    "        # define shared weights\n",
    "        self.TransMet = nn.Parameter(torch.randn(attr_size, ft_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.DenseL(x)\n",
    "        out = self.AttnL(out)\n",
    "        out = self.Conv1dL(out).squeeze()\n",
    "        out = self.DenseL2(out)\n",
    "        # SAE Operation\n",
    "        self.EncDenseL.weight = self.TransMet\n",
    "        self.DecDenseL.weight = self.TransMet.T \n",
    "        attr_out = self.EncDenseL(out)\n",
    "        ft_out = self.DecDenseL(attr_out)\n",
    "        return attr_out, ft_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 32]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model \n",
    "sample_input = torch.randn((32, 120, 42))\n",
    "model = Model3(in_ft=42, d_model=128, num_heads=2, ft_size=64, attr_size=32,  max_len=120, dropout=0.1)\n",
    "sample_output = model(sample_input)\n",
    "sample_output[0].shape, sample_output[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Rel.Glob.Attn. > 1DConv > SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model4(nn.Module):\n",
    "    def __init__(self, in_ft, d_model, num_heads, ft_size, max_len=1024, dropout=0.1):\n",
    "        super(Model4, self).__init__()\n",
    "        self.in_ft = in_ft\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ft_size = ft_size \n",
    "\n",
    "        # feature prep layer\n",
    "        self.DenseL = nn.Linear(in_ft, d_model)\n",
    "        # relative global attention layer\n",
    "        self.AttnL = RelativeGlobalAttention(self.d_model, self.num_heads, self.max_len)\n",
    "        # positional encoding concat <-> 1DConv \n",
    "        self.Act = F.relu#_get_activation_fn(activation)\n",
    "        self.Conv1dL = nn.Conv1d(self.max_len, 1, 1)\n",
    "        self.DenseL2 = nn.Linear(self.d_model, self.ft_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.DenseL(x)\n",
    "        out = self.AttnL(out)\n",
    "        out = self.Conv1dL(out).squeeze()\n",
    "        out = self.DenseL2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 120, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model \n",
    "sample_input = torch.randn((32, 120, 42))\n",
    "model = Model4(in_ft=42, d_model=128, num_heads=2, ft_size=64, max_len=120, dropout=0.1)\n",
    "sample_output = model(sample_input)\n",
    "sample_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Rel.Glob.Attn > Glob.Pool > SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model5(nn.Module):\n",
    "    def __init__(self, in_ft, d_model, num_heads, ft_size, attr_size, max_len=1024, dropout=0.1):\n",
    "        super(Model5, self).__init__()\n",
    "        self.in_ft = in_ft\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ft_size = ft_size \n",
    "\n",
    "        # feature prep layer\n",
    "        self.DenseL = nn.Linear(in_ft, d_model)\n",
    "        # relative global attention layer\n",
    "        self.AttnL = RelativeGlobalAttention(self.d_model, self.num_heads, self.max_len)\n",
    "        # positional encoding concat <-> 1DConv \n",
    "        self.Act = F.relu#_get_activation_fn(activation)\n",
    "        self.AvgPoolL = nn.AvgPool2d((self.max_len,1))\n",
    "        self.DenseL2 = nn.Linear(self.d_model, self.ft_size)\n",
    "        self.dropout = nn.Dropout1d(dropout)\n",
    "        # SAE submodule\n",
    "        self.EncDenseL = nn.Linear(in_features=ft_size, out_features=attr_size, bias=False)\n",
    "        self.DecDenseL = nn.Linear(in_features=attr_size, out_features=ft_size, bias=False)\n",
    "        # override weights\n",
    "        del self.EncDenseL.weight\n",
    "        del self.DecDenseL.weight\n",
    "        # define shared weights\n",
    "        self.TransMet = nn.Parameter(torch.randn(attr_size, ft_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.DenseL(x)\n",
    "        out = self.AttnL(out)\n",
    "        out = self.Act(out)\n",
    "        out = self.AvgPoolL(out)\n",
    "        out = torch.squeeze(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.DenseL2(out)\n",
    "        # SAE Operation\n",
    "        self.EncDenseL.weight = self.TransMet\n",
    "        self.DecDenseL.weight = self.TransMet.T \n",
    "        attr_out = self.EncDenseL(out)\n",
    "        ft_out = self.DecDenseL(attr_out)\n",
    "        \n",
    "        return attr_out, ft_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 32]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model \n",
    "sample_input = torch.randn((32, 120, 42))\n",
    "model = Model5(in_ft=42, d_model=128, num_heads=2, ft_size=64, attr_size=32,  max_len=120, dropout=0.1)\n",
    "sample_output = model(sample_input)\n",
    "sample_output[0].shape, sample_output[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Rel.Glob.Attn > Glob.Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model6(nn.Module):\n",
    "    def __init__(self, in_ft, d_model, num_heads, ft_size, max_len=1024, dropout=0.1):\n",
    "        super(Model6, self).__init__()\n",
    "        self.in_ft = in_ft\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ft_size = ft_size \n",
    "\n",
    "        # feature prep layer\n",
    "        self.DenseL = nn.Linear(in_ft, d_model)\n",
    "        # relative global attention layer\n",
    "        self.AttnL = RelativeGlobalAttention(self.d_model, self.num_heads, self.max_len)\n",
    "        # positional encoding concat <-> 1DConv \n",
    "        self.Act = F.relu#_get_activation_fn(activation)\n",
    "        self.AvgPoolL = nn.AvgPool2d((self.max_len,1))\n",
    "        self.DenseL2 = nn.Linear(self.d_model, self.ft_size)\n",
    "        self.dropout = nn.Dropout1d(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.DenseL(x)\n",
    "        out = self.AttnL(out)\n",
    "        out = self.Act(out)\n",
    "        out = self.AvgPoolL(out)\n",
    "        out = torch.squeeze(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.DenseL2(out)        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model \n",
    "sample_input = torch.randn((32, 120, 42))\n",
    "model = Model6(in_ft=42, d_model=128, num_heads=2, ft_size=64, max_len=120, dropout=0.1)\n",
    "sample_output = model(sample_input)\n",
    "sample_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70976"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Attn. > LSTM > SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_encoder(pos_encoding):\n",
    "    if pos_encoding == \"learnable\":\n",
    "        return LearnablePositionalEncoding\n",
    "    elif pos_encoding == \"fixed\":\n",
    "        return FixedPositionalEncoding\n",
    "\n",
    "    raise NotImplementedError(\"pos_encoding should be 'learnable'/'fixed', not '{}'\".format(pos_encoding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model7(nn.Module):\n",
    "    def __init__(self, in_ft, d_model, num_heads, num_layers, dim_feedforward, ft_size, attr_size,\n",
    "                pos_encoding='fixed', activation='gelu', norm='BatchNorm', max_len=1024, dropout=0.1, freeze=False):\n",
    "        super(Model7, self).__init__()\n",
    "        self.in_ft = in_ft\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ft_size = ft_size \n",
    "        self.attr_size = attr_size\n",
    "\n",
    "        # feature prep layer\n",
    "        self.DenseL = nn.Linear(in_ft, d_model)\n",
    "        self.pos_enc = get_pos_encoder(pos_encoding)(d_model, dropout=dropout*(1.0 - freeze), max_len=max_len)\n",
    "        # relative global attention layer\n",
    "        if norm == 'LayerNorm':\n",
    "            encoder_layer = TransformerEncoderLayer(d_model, self.num_heads, dim_feedforward, dropout*(1.0 - freeze), activation=activation)\n",
    "        else:\n",
    "            encoder_layer = TransformerBatchNormEncoderLayer(d_model, self.num_heads, dim_feedforward, dropout*(1.0 - freeze), activation=activation)\n",
    "\n",
    "        self.AttnL = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        # positional encoding concat <-> LSTM \n",
    "        self.lstmL = nn.LSTM(input_size=d_model, hidden_size=ft_size, batch_first=True)\n",
    "        # SAE submodule\n",
    "        self.EncDenseL = nn.Linear(in_features=ft_size, out_features=attr_size, bias=False)\n",
    "        self.DecDenseL = nn.Linear(in_features=attr_size, out_features=ft_size, bias=False)\n",
    "        # override weights\n",
    "        del self.EncDenseL.weight\n",
    "        del self.DecDenseL.weight\n",
    "        # define shared weights\n",
    "        self.TransMet = nn.Parameter(torch.randn(attr_size, ft_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1,0,2)\n",
    "        out = self.DenseL(x)\n",
    "        out = self.pos_enc(out)* math.sqrt(self.d_model)\n",
    "        out = self.AttnL(out)\n",
    "        out = out.permute(1,0,2)\n",
    "        lstm_out, hidden = self.lstmL(out)\n",
    "        # SAE Operation\n",
    "        self.EncDenseL.weight = self.TransMet\n",
    "        self.DecDenseL.weight = self.TransMet.T \n",
    "        attr_out = self.EncDenseL(lstm_out[:, -1, :])\n",
    "        ft_out = self.DecDenseL(attr_out)\n",
    "        return attr_out, ft_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 32]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model \n",
    "sample_input = torch.randn((32, 120, 42))\n",
    "model = Model7(in_ft=42, d_model=128, num_heads=2, dim_feedforward=128, \n",
    "            num_layers=2, ft_size=64, attr_size=32,  max_len=120, pos_encoding='fixed', activation='gelu', norm='BatchNorm', dropout=0.1)\n",
    "sample_output = model(sample_input)\n",
    "sample_output[0].shape, sample_output[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Attn. > LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model8(nn.Module):\n",
    "    def __init__(self, in_ft, d_model, num_heads, num_layers, dim_feedforward, ft_size,\n",
    "                pos_encoding='fixed', activation='gelu', norm='BatchNorm', max_len=1024, dropout=0.1, freeze=False):\n",
    "        super(Model8, self).__init__()\n",
    "        self.in_ft = in_ft\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ft_size = ft_size \n",
    "\n",
    "        # feature prep layer\n",
    "        self.DenseL = nn.Linear(in_ft, d_model)\n",
    "        self.pos_enc = get_pos_encoder(pos_encoding)(d_model, dropout=dropout*(1.0 - freeze), max_len=max_len)\n",
    "        # relative global attention layer\n",
    "        if norm == 'LayerNorm':\n",
    "            encoder_layer = TransformerEncoderLayer(d_model, self.num_heads, dim_feedforward, dropout*(1.0 - freeze), activation=activation)\n",
    "        else:\n",
    "            encoder_layer = TransformerBatchNormEncoderLayer(d_model, self.num_heads, dim_feedforward, dropout*(1.0 - freeze), activation=activation)\n",
    "\n",
    "        self.AttnL = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        # positional encoding concat <-> LSTM \n",
    "        self.lstmL = nn.LSTM(input_size=d_model, hidden_size=ft_size, batch_first=True)\n",
    "        self.DensePostL = nn.Linear(in_features=ft_size, out_features=ft_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1,0,2)\n",
    "        out = self.DenseL(x)\n",
    "        out = self.pos_enc(out)* math.sqrt(self.d_model)\n",
    "        out = self.AttnL(out)\n",
    "        out = out.permute(1,0,2)\n",
    "        lstm_out, hidden = self.lstmL(out)\n",
    "        out = self.DensePostL(lstm_out[:, -1, :])\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model \n",
    "sample_input = torch.randn((32, 120, 42))\n",
    "model = Model8(in_ft=42, d_model=128, num_heads=2, dim_feedforward=128, \n",
    "            num_layers=2, ft_size=64, max_len=120, pos_encoding='fixed', activation='gelu', norm='BatchNorm', dropout=0.1)\n",
    "sample_output = model(sample_input)\n",
    "sample_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Build Ideal Model Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    # attention encoder\n",
    "    'encoder': 'self-attn', # 'rel-glob-attn' \n",
    "    # positional encoding \n",
    "    'posi-encoder': 'fixed', # 'learnable' \n",
    "    # encoder post feature processing model\n",
    "    'post-encoder': 'conv', # 'linear', 'lstm' \n",
    "    'post-config': {}, # based on the \"post-encoder\" type \n",
    "    'add-SAE': True,\n",
    "    # encoder I/O sizes\n",
    "    'max_len': 120,\n",
    "    'in-feat': 128,\n",
    "    'out-feat': 64,\n",
    "    'attr_feat': 32,\n",
    "    # attn. layer config\n",
    "    'd_model': 51,\n",
    "    'n_heads': 3,\n",
    "    'dim_feadforward': 128,\n",
    "    'dropout': 0.2,\n",
    "    'activation': 'relu',\n",
    "    'norm': 'layerNorm', # 'batchNorm'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import random \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-0.10.1-py3-none-any.whl (529 kB)\n",
      "     ------------------------------------ 529.6/529.6 kB 405.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages (from torchmetrics) (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages (from torchmetrics) (1.23.1)\n",
      "Requirement already satisfied: torch>=1.3.1 in c:\\users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages (from torchmetrics) (1.12.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages (from packaging->torchmetrics) (3.0.9)\n",
      "Installing collected packages: torchmetrics\n",
      "Successfully installed torchmetrics-0.10.1\n"
     ]
    }
   ],
   "source": [
    "! pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import pairwise_cosine_similarity, pairwise_euclidean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = torch.rand((32, 42), requires_grad=True)\n",
    "action_mat = torch.rand((18,42))\n",
    "action_class = torch.from_numpy(np.array(random.choices(range(18), k=32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cos_mat \u001b[39m=\u001b[39m pairwise_cosine_similarity(model_out, action_mat)\n\u001b[0;32m      2\u001b[0m cos_mat\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\torchmetrics\\functional\\pairwise\\cosine.py:88\u001b[0m, in \u001b[0;36mpairwise_cosine_similarity\u001b[1;34m(x, y, reduction, zero_diagonal)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpairwise_cosine_similarity\u001b[39m(\n\u001b[0;32m     48\u001b[0m     x: Tensor,\n\u001b[0;32m     49\u001b[0m     y: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     50\u001b[0m     reduction: Literal[\u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     51\u001b[0m     zero_diagonal: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     52\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m     53\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Calculates pairwise cosine similarity:\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[0;32m     55\u001b[0m \u001b[39m    .. math::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39m                [0.9996, 0.9998, 0.0000]])\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m     distance \u001b[39m=\u001b[39m _pairwise_cosine_similarity_update(x, y, zero_diagonal)\n\u001b[0;32m     89\u001b[0m     \u001b[39mreturn\u001b[39;00m _reduce_distance_matrix(distance, reduction)\n",
      "File \u001b[1;32mc:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\torchmetrics\\functional\\pairwise\\cosine.py:37\u001b[0m, in \u001b[0;36m_pairwise_cosine_similarity_update\u001b[1;34m(x, y, zero_diagonal)\u001b[0m\n\u001b[0;32m     34\u001b[0m x, y, zero_diagonal \u001b[39m=\u001b[39m _check_input(x, y, zero_diagonal)\n\u001b[0;32m     36\u001b[0m norm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnorm(x, p\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m x \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m norm\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m     38\u001b[0m norm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnorm(y, p\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     39\u001b[0m y \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m norm\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "cos_mat = pairwise_cosine_similarity(model_out, action_mat)\n",
    "cos_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 45)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_data = np.load('../data/DaLiAc_Dataset/attribute_ft.npy')\n",
    "np_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 29)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_data = np.load('../data/UTD-MHAD-Inertial/attribute_ft.npy')\n",
    "np_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_data = np.load('../data/OPP_Dataset/attribute_ft.npy')\n",
    "np_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mvts_trans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
